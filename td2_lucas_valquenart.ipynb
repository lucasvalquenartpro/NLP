{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI7WIbotzb2j"
   },
   "source": [
    "# <center>Natural Language Processing Hands-on #2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1W1M392zb2k"
   },
   "source": [
    "During the Natural Language Processing course, text representation algorithms have been introduced. However they don't suffice to the creation of complete NLP systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgzQEH1dzb2k"
   },
   "source": [
    "Most of them usually rely on text pre-processing at first -- in other words, they rely on a specific data pipeline that is tied to the final task you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vx1_dVkzb2k"
   },
   "source": [
    "As a result, we will try in this notebook to create a pipeline from scratch given a specific final task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mezA-O2Fzb2k"
   },
   "source": [
    "# Resources you'll need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5CfCgcAzb2k"
   },
   "source": [
    "## Machine Learning libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fMtvvXMzb2l"
   },
   "source": [
    "Lots of ML libraries exist in the wild. You have general libraries such as [scikit-learn](https://scikit-learn.org/stable/), domain related libraries such as [nltk](https://www.nltk.org/) or hyper specific implementation of optimized algorithms such as annoy [annoy](https://pypi.org/project/annoy/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO8INPg1zb2l"
   },
   "source": [
    "In this notebook, you'll need to rely on the following packages:\n",
    "\n",
    "   - [scikit-learn](https://scikit-learn.org/stable/): all purpose machine learning resource if they aren't neural based.\n",
    "   - [nltk](https://www.nltk.org/): natural language toolkit -- implements lots of preprocessing steps and text transformation.\n",
    "   - [gensim](https://radimrehurek.com/gensim/): library designed to be easy to use for both topic modeling and text representation.\n",
    "   - [spacy](https://spacy.io/): industrialization machine learning systems. Provide lots of pretrained weights for various models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCC8mhkxzb2l"
   },
   "source": [
    "Usually, a simple pip install is sufficient for them to work. If you have already installed it, feel free to create a dedicated virtual environment, which is really a good practice. If you want to know more regarding that, you can rely on this [here](https://virtualenvwrapper.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz8GOU4ezb2l"
   },
   "source": [
    "## Data & final task definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPkANuCtzb2l"
   },
   "source": [
    "Given the [News dataset](https://www.kaggle.com/rmisra/news-category-dataset/download) (also available alongside this notebook), you'll have to build a simple topic modeling system that will identify the topics of the news headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_J4g3Apzb2l"
   },
   "source": [
    "Those headlines have already been labelled. Here are the categories and document counts of this dataset:\n",
    "\n",
    "* POLITICS: 32739\n",
    "\n",
    "* WELLNESS: 17827\n",
    "\n",
    "* ENTERTAINMENT: 16058\n",
    "\n",
    "* TRAVEL: 9887\n",
    "\n",
    "* STYLE & BEAUTY: 9649\n",
    "\n",
    "* PARENTING: 8677\n",
    "\n",
    "* HEALTHY LIVING: 6694\n",
    "\n",
    "* QUEER VOICES: 6314\n",
    "\n",
    "* FOOD & DRINK: 6226\n",
    "\n",
    "* BUSINESS: 5937\n",
    "\n",
    "* COMEDY: 5175\n",
    "\n",
    "* SPORTS: 4884\n",
    "\n",
    "* BLACK VOICES: 4528\n",
    "\n",
    "* HOME & LIVING: 4195\n",
    "\n",
    "* PARENTS: 3955\n",
    "\n",
    "* THE WORLDPOST: 3664\n",
    "\n",
    "* WEDDINGS: 3651\n",
    "\n",
    "* WOMEN: 3490\n",
    "\n",
    "* IMPACT: 3459\n",
    "\n",
    "* DIVORCE: 3426\n",
    "\n",
    "* CRIME: 3405\n",
    "\n",
    "* MEDIA: 2815\n",
    "\n",
    "* WEIRD NEWS: 2670\n",
    "\n",
    "* GREEN: 2622\n",
    "\n",
    "* WORLDPOST: 2579\n",
    "\n",
    "* RELIGION: 2556\n",
    "\n",
    "* STYLE: 2254\n",
    "\n",
    "* SCIENCE: 2178\n",
    "\n",
    "* WORLD NEWS: 2177\n",
    "\n",
    "* TASTE: 2096\n",
    "\n",
    "* TECH: 2082\n",
    "\n",
    "* MONEY: 1707\n",
    "\n",
    "* ARTS: 1509\n",
    "\n",
    "* FIFTY: 1401\n",
    "\n",
    "* GOOD NEWS: 1398\n",
    "\n",
    "* ARTS & CULTURE: 1339\n",
    "\n",
    "* ENVIRONMENT: 1323\n",
    "\n",
    "* COLLEGE: 1144\n",
    "\n",
    "* LATINO VOICES: 1129\n",
    "\n",
    "* CULTURE & ARTS: 1030\n",
    "\n",
    "* EDUCATION: 1004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM1pCRfSzb2l"
   },
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WaVDsEMTzb2m",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:41.752076Z",
     "start_time": "2025-11-26T18:38:41.383136Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUjqZkXazb2l",
    "outputId": "c5746eae-8214-47db-d73e-466353b41a05"
   },
   "cell_type": "code",
   "source": "#!pip3 install pandas\n",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH960wEczb2m"
   },
   "source": [
    "You can load the dataset using pandas and the [.read_json()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html) method. Try loading your dataset here:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wf-DdbNWzb2m",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:44.128169Z",
     "start_time": "2025-11-26T18:38:43.650078Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "dataset_data = []\n",
    "with open(\"News_Category_Dataset_v2.json\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            dataset_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping malformed JSON line {i+1}: {line.strip()}. Error: {e}\")\n",
    "\n",
    "dataset = pd.DataFrame(dataset_data)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "kN13rhplzb2m",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:44.689117Z",
     "start_time": "2025-11-26T18:38:44.687296Z"
    }
   },
   "source": [
    "dataset = dataset.head(1000)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "#dataset.head()",
   "metadata": {
    "id": "QAQGZNHM0C-u",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "outputId": "a858878a-d123-4b77-8a76-6ebe99637ae2",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:45.593109Z",
     "start_time": "2025-11-26T18:38:45.590664Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf1oyLEYzb2m"
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVlXfVH1zb2m"
   },
   "source": [
    "A good way to get the grasp of your corpus is to count the occurences of words across it. For convenience, we've defined a dummy function that splits words by checking where spaces are and... Simply that. This is the most basic form of word identification in text that could be used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0285BJepzb2m",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:48.293703Z",
     "start_time": "2025-11-26T18:38:48.291693Z"
    }
   },
   "source": [
    "def dummy_word_split(texts):\n",
    "    \"\"\"Function identifying words in a sentence in a really dummy way.\n",
    "\n",
    "        Argument:\n",
    "            - texts (list of str): a list of raw texts in which we'd like to identify words\n",
    "\n",
    "        Return:\n",
    "            - list of list containing each word separately.\n",
    "    \"\"\"\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        texts_out.append(text.split(\" \"))\n",
    "\n",
    "    return texts_out"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aCuocQ4Nzb2m",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:49.083702Z",
     "start_time": "2025-11-26T18:38:49.080122Z"
    }
   },
   "source": [
    "splitted_texts = dummy_word_split(dataset[\"headline\"].tolist())"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:49.550146Z",
     "start_time": "2025-11-26T18:38:49.548688Z"
    }
   },
   "cell_type": "code",
   "source": "#dataset[\"headline\"]",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Tn2pNkbzb2m",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:50.721297Z",
     "start_time": "2025-11-26T18:38:50.718881Z"
    }
   },
   "source": "#splitted_texts[0]",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCtY5qELzb2m"
   },
   "source": [
    "Now, let's define a function that counts word occurences and highlight what are the most important words of our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eSkUNJcvzb2m",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:52.525535Z",
     "start_time": "2025-11-26T18:38:52.523645Z"
    }
   },
   "source": [
    "def compute_word_occurences(texts):\n",
    "    word_occurences = {}\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word in word_occurences:\n",
    "                word_occurences[word] += 1\n",
    "            else:\n",
    "                word_occurences[word] = 1\n",
    "    return word_occurences"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VreprMZ1zb2m"
   },
   "source": [
    "Once this is done, display the top 20 most occuring words in your texts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "gHKGAB4bzb2m",
    "outputId": "65353eba-34bd-4c7c-b811-b35d96c153d5",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:38:57.113708Z",
     "start_time": "2025-11-26T18:38:57.111196Z"
    }
   },
   "source": "#pd.Series(compute_word_occurences(splitted_texts)).sort_values(ascending=False).head(20)",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0Bw1mlCzb2n"
   },
   "source": "### Does it make sense, and can you leverage such results?"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Yes and no. The results show the most frequent words, but they are dominated by stop words (very common but uninformative words) like \"To\", \"The\", \"Of\", \"In\", \"A\", \"And\", etc.\n",
    "These words appear everywhere and tell us nothing about the actual content of the texts. Only a few words like \"Trump\", \"Donald\", \"New\", \"Says\" are truly informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyXaHnbzzb2n"
   },
   "source": "# Actual pipeline"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWHr5R8Ezb2n"
   },
   "source": [
    "As you have seen above, the results obtained from a simple word count aren't so great. Similar words doesn't add up (such as run and running), and you have a lot of noise included. Words such as *the*, *you*, *an* could be removed for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-18udZGzb2n"
   },
   "source": [
    "Actually, a lot can be done. Let's check that out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj8bzixmzb2n"
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXLAtSBJzb2n"
   },
   "source": [
    "## What does the pipeline look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH_4VShOzb2n"
   },
   "source": [
    "A NLP data pipeline often relies on the following elements. Some can be added, some can be removed, but they all look like this at some point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6sr9LHQzb2n"
   },
   "source": [
    "1. **Ensuring data quality.** You have to make sure that there's no N/A in your data and that everything is in the good format shape. Having this as the entrance of your pipeline will save you a lot of time in the long run, so try defining it thoroughly.\n",
    "\n",
    "\n",
    "2. **Filtering texts from unwanted characters**. Especially if you get data from web, you'll end up with HTML tags or encoding stuff that you don't need in your texts. Before applying anything to them, you need to get them cleaned up. Here, try removing the dates and the punctuation for instance.\n",
    "\n",
    "\n",
    "3. **Unify your texts**. (*This is topic modeling specific*). You don't want to make the difference between a word at the beginning of a phrase of in the middle of it here. You should unify all your words by lowercasing them and deaccenting them as well.\n",
    "\n",
    "\n",
    "4. **Converting sentences to lists of words**. Some words aren't needed for our analyses, such as *your*, *my*, etc. In order to remove them easily, you have to convert your sentences to lists of words. You can use the dummy function defined above but I'd advised against it. Try finding a function that does that smoothly in [gensim.utils](https://radimrehurek.com/gensim/utils.html)!\n",
    "\n",
    "\n",
    "5. **Remove useless words**. You need to remove useless words from your corpus. You have two approaches: [use a hard defined list of stopwords](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/) or rely on TF-IDF to identify useless words. The first is the simplest, the second might yield better results!\n",
    "\n",
    "\n",
    "6. **Creating n-grams**. If you look at New York, it is composed of two words. As a result, a word count wouldn't really return a true count for *New York* per se. In NLP, we represent New York as New_York, which is considered a single word. The n-gram creation consists in identifying words that occur together often and regrouping them. It boosts interpretability for topic modeling in this case.\n",
    "\n",
    "\n",
    "7. **Stemming / Lemmatization**. Shouldn't run, running, runnable be grouped and counted as a single word when we're identifying discussion topics? Yes, they should. Stemming is the process of cutting words to their word root (run- for instance) quite brutally while lemmatization will do the same by identifying the kind of word it is working on. You should convert the corpus words into those truncated representations to have a more realistic word count.\n",
    "\n",
    "\n",
    "8. **Part of speech tagging**. POS helps in the identification of verbs, nouns, adjectives, etc. For topic models, it is a good idea to work only on verbs and nouns. Adjectives don't convey info about the actual underlying topic discussed at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LSfip1Gzb2n"
   },
   "source": [
    "## Let's create it!"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip3 install gensim\n",
    "#!pip3 install spacy"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWTQU3jS0Vrx",
    "outputId": "6ae9296f-92f1-4be6-d5c2-be0ecce33c98",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:14:14.306777Z",
     "start_time": "2025-11-26T18:10:36.748966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting spacy\r\n",
      "  Downloading spacy-3.8.11.tar.gz (1.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m42.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\r\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\r\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\r\n",
      "  Using cached murmurhash-1.0.15-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.3 kB)\r\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\r\n",
      "  Using cached cymem-2.0.13-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.7 kB)\r\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\r\n",
      "  Using cached preshed-3.0.12-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.5 kB)\r\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\r\n",
      "  Using cached thinc-8.3.9-cp39-cp39-macosx_10_9_universal2.whl\r\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\r\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\r\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\r\n",
      "  Using cached srsly-2.5.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (19 kB)\r\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\r\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\r\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\r\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from spacy) (4.67.1)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from spacy) (1.24.3)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from spacy) (2.32.5)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from spacy) (2.12.4)\r\n",
      "Requirement already satisfied: jinja2 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from spacy) (3.1.6)\r\n",
      "Requirement already satisfied: setuptools in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from spacy) (44.1.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from spacy) (25.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\r\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\r\n",
      "  Using cached blis-1.3.3-cp39-cp39-macosx_10_9_universal2.whl\r\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\r\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\r\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\r\n",
      "  Using cached cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from jinja2->spacy) (3.0.2)\r\n",
      "Requirement already satisfied: wrapt in /Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\r\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\r\n",
      "Using cached cymem-2.0.13-cp39-cp39-macosx_11_0_arm64.whl (43 kB)\r\n",
      "Using cached murmurhash-1.0.15-cp39-cp39-macosx_11_0_arm64.whl (27 kB)\r\n",
      "Using cached preshed-3.0.12-cp39-cp39-macosx_11_0_arm64.whl (126 kB)\r\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\r\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\r\n",
      "Using cached srsly-2.5.2-cp39-cp39-macosx_11_0_arm64.whl (654 kB)\r\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\r\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\r\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\r\n",
      "Using cached cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\r\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\r\n",
      "Building wheels for collected packages: spacy\r\n",
      "  Building wheel for spacy (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for spacy: filename=spacy-3.8.11-cp39-cp39-macosx_10_9_universal2.whl size=11518999 sha256=cef5d621fcf090ceddc761f670c3835a833c73ed8f72be5189a439e872697934\r\n",
      "  Stored in directory: /Users/lucasvalquenart/Library/Caches/pip/wheels/a8/ad/31/4477e5bf199f83e76944b01dce246860fa577d6ee5126300e8\r\n",
      "Successfully built spacy\r\n",
      "Installing collected packages: wasabi, typer-slim, spacy-loggers, spacy-legacy, murmurhash, cymem, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\r\n",
      "\u001B[33m  WARNING: The script weasel is installed in '/Users/lucasvalquenart/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The script spacy is installed in '/Users/lucasvalquenart/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0mSuccessfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.9 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tW3PgmQuzb2n",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:39:05.607299Z",
     "start_time": "2025-11-26T18:39:03.153992Z"
    }
   },
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import secrets\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec, Phrases, KeyedVectors\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from spacy.parts_of_speech import IDS as POS_map"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXKhIF6izb2n"
   },
   "source": [
    "Now it's your turn. Try to implement each step of the pipeline, and compare the word counts obtained earlier and the one obtained after preprocessing your texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymphl3F2zb2n"
   },
   "source": [
    "### Ensuring data quality"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q4Aw83tRzb2w",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:39:07.344136Z",
     "start_time": "2025-11-26T18:39:07.342072Z"
    }
   },
   "source": [
    "def check_data_quality(texts):\n",
    "    \"\"\"Check wheter all the dataset is conform to the expected behaviour.\"\"\"\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word is None:\n",
    "                force_format(word)\n",
    "    return True"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y-maQi8szb2w",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:39:08.586075Z",
     "start_time": "2025-11-26T18:39:08.584247Z"
    }
   },
   "source": [
    "def force_format(texts):\n",
    "    return [str(t) for t in texts]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H_QXVYrMzb2w",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:40:54.174271Z",
     "start_time": "2025-11-26T18:40:54.171589Z"
    }
   },
   "source": "texts = force_format(dataset[\"headline\"])",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWJnISffzb2w",
    "outputId": "76d36538-a4f9-447a-f4eb-7ff648fe3f46",
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T18:39:09.256443Z",
     "start_time": "2025-11-26T18:39:09.254211Z"
    }
   },
   "source": "print(f\"data quality check?\\n{check_data_quality(texts)}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data quality check?\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTWIQYADzb2w"
   },
   "source": [
    "### Filtering texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOGR1Exnzb2x"
   },
   "source": [
    "https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ai3tuRWIzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:39:11.815208Z",
     "start_time": "2025-11-26T18:39:11.812898Z"
    }
   },
   "source": [
    "def filter_text(texts_in):\n",
    "    \"\"\"Removes incorrect patterns from a list of texts\"\"\"\n",
    "    clean_texts = []\n",
    "    for text in texts_in:\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', '', text)\n",
    "        text = re.sub(r'\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        clean_texts.append(text)\n",
    "\n",
    "    return clean_texts"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T18:40:56.494013Z",
     "start_time": "2025-11-26T18:40:56.491312Z"
    }
   },
   "cell_type": "code",
   "source": "texts[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JsTLZAmgzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:40:59.815059Z",
     "start_time": "2025-11-26T18:40:59.801499Z"
    }
   },
   "source": [
    "texts = filter_text(texts)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": "texts[0]",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": true,
    "id": "yAd6fRfV-nf7",
    "outputId": "594287ad-6bdb-45e4-8b64-694c456f23a3",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:41:00.164512Z",
     "start_time": "2025-11-26T18:41:00.162089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There Were  Mass Shootings In Texas Last Week  But Only  On TV'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5DwbVk6zb2x"
   },
   "source": [
    "### Unifying texts & converting sentences to list of words"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip3 install unidecode"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cu0ZE8bj0zg7",
    "outputId": "0c94d03c-ff50-4ec2-dda0-63fe12013ef3",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:23:03.961625Z",
     "start_time": "2025-11-26T18:23:03.035522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting unidecode\r\n",
      "  Using cached Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Using cached Unidecode-1.4.0-py3-none-any.whl (235 kB)\r\n",
      "Installing collected packages: unidecode\r\n",
      "\u001B[33m  WARNING: The script unidecode is installed in '/Users/lucasvalquenart/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0mSuccessfully installed unidecode-1.4.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fisNu9L0zb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:39:20.212343Z",
     "start_time": "2025-11-26T18:39:20.208119Z"
    }
   },
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    \"\"\"Converts sentences to words.\n",
    "\n",
    "    Convert sentences in lists of words while removing the accents and the punctuation.\n",
    "\n",
    "    @param:\n",
    "        sentences: a list of strings, the sentences we want to convert\n",
    "    @return\n",
    "        A list of words' lists.\n",
    "    \"\"\"\n",
    "    unified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = unidecode(sentence)\n",
    "        unified_sentences.append(sentence)\n",
    "\n",
    "    return unified_sentences"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SN2g5NVHzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:11.517826Z",
     "start_time": "2025-11-26T18:44:11.496818Z"
    }
   },
   "source": [
    "words = sent_to_words(texts)\n",
    "words = [simple_preprocess(word, deacc=True) for word in words]"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "source": "words[0]",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rRimNFQY_V6M",
    "outputId": "0a248a2c-4fd0-4b50-b977-38dba17af7ee",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:12.582793Z",
     "start_time": "2025-11-26T18:44:12.580418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there',\n",
       " 'were',\n",
       " 'mass',\n",
       " 'shootings',\n",
       " 'in',\n",
       " 'texas',\n",
       " 'last',\n",
       " 'week',\n",
       " 'but',\n",
       " 'only',\n",
       " 'on',\n",
       " 'tv']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4aieAJmzb2x"
   },
   "source": [
    "### Removing useless words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m7NMpjgizb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:15.269428Z",
     "start_time": "2025-11-26T18:44:15.265490Z"
    }
   },
   "source": [
    "def get_stopwords(additional_stopwords=[]):\n",
    "    \"\"\"Return a list of english stopwords, that can be augmented by using a stopwords file or a list of stopwords\n",
    "\n",
    "    Args:\n",
    "        filepath (str, optional): path to a text file where each line is a stopword\n",
    "        additional_stopwords (list of str, optional): list of string representing stopwords\n",
    "    Returns:\n",
    "        List of strings representing stopwords\n",
    "    \"\"\"\n",
    "\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stop_w= f.readlines()\n",
    "\n",
    "    stopwords = [s.rstrip() for s in stop_w]\n",
    "    stopwords = list(text.ENGLISH_STOP_WORDS.union(stopwords) )\n",
    "    if additional_stopwords:\n",
    "        stopwords += additional_stopwords\n",
    "    stopwords = list(set(stopwords))\n",
    "    stopwords = [s.replace(\"\\n\", \"\") for s in stopwords]\n",
    "    stopwords = sorted(stopwords, key=str. lower)\n",
    "    return stopwords"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xAL_-Ckzb2x",
    "outputId": "504fff1f-ada3-40a0-8676-184fe63f38f0",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:16.220096Z",
     "start_time": "2025-11-26T18:44:16.169493Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "stopwords = get_stopwords()\n",
    "\n",
    "words = [[word for word in wrd if word not in stopwords] for wrd in tqdm(words)]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 21670.28it/s]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "source": "words[0]",
   "metadata": {
    "id": "0FdBo9MV2S57",
    "outputId": "86faceb2-d109-45a4-9f06-02ddce2d4ef8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:18.674398Z",
     "start_time": "2025-11-26T18:44:18.671605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mass', 'shootings', 'texas', 'week', 'tv']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAxCmJpjzb2x"
   },
   "source": [
    "### Creating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-U3ZRhRLzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:21.224614Z",
     "start_time": "2025-11-26T18:44:21.218902Z"
    }
   },
   "source": [
    "def create_bigrams(texts, bigram_count=15, threshold=10, convert_sent_to_words=False, as_str=True):\n",
    "    \"\"\"Identify bigrams in texts and return the texts with bigrams integrated\"\"\"\n",
    "    if convert_sent_to_words:\n",
    "        texts = [simple_preprocess(text) for text in texts]\n",
    "    bigram = Phrases(texts, min_count=bigram_count, threshold=threshold)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    texts_with_bigrams = [bigram_mod[doc] for doc in texts]\n",
    "    if as_str:\n",
    "        texts_with_bigrams = [' '.join(doc) for doc in texts_with_bigrams]\n",
    "    return texts_with_bigrams\n",
    "\n",
    "def create_trigrams(texts, trigram_count=15, threshold=10, convert_sent_to_words=False, as_str=True):\n",
    "    \"\"\"Identify trigrams in texts and return the texts with trigrams integrated\"\"\"\n",
    "    if convert_sent_to_words:\n",
    "        texts = [simple_preprocess(text) for text in texts]\n",
    "    bigram = Phrases(texts, min_count=trigram_count, threshold=threshold)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    texts_with_bigrams = [bigram_mod[doc] for doc in texts]\n",
    "    trigram = Phrases(texts_with_bigrams, min_count=trigram_count, threshold=threshold)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    texts_with_trigrams = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    if as_str:\n",
    "        texts_with_trigrams = [' '.join(doc) for doc in texts_with_trigrams]\n",
    "\n",
    "    return texts_with_trigrams"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:30.865860Z",
     "start_time": "2025-11-26T18:44:30.862085Z"
    }
   },
   "cell_type": "code",
   "source": "words[4]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['julianna', 'margulies', 'donald', 'trump', 'poop', 'bags', 'pick', 'dog']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nIcIyiFAzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:41.588074Z",
     "start_time": "2025-11-26T18:44:41.566993Z"
    }
   },
   "source": [
    "words = create_bigrams(words, as_str= False)"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "source": "words[4]",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR6mzoMR-aqL",
    "outputId": "b0e3fa0c-8289-425d-e684-3fff38115101",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:44:42.406209Z",
     "start_time": "2025-11-26T18:44:42.403064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['julianna', 'margulies', 'donald_trump', 'poop', 'bags', 'pick', 'dog']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here donald_trump is a bigrams"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWfScKlIzb2x"
   },
   "source": [
    "### Stemming / Lemmatization & Part-of-Speech filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tBsOxJLzb2x"
   },
   "source": [
    "***Note***: *if you encounter an error regarding a missing spacy model, head to your CLI and enter*\n",
    "````bash\n",
    "    python -m spacy download en_core_web_md\n",
    "````"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T18:46:42.324727Z",
     "start_time": "2025-11-26T18:46:40.303351Z"
    }
   },
   "cell_type": "code",
   "source": "!python3 -m spacy download en_core_web_sm",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting en-core-web-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m139.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: en-core-web-sm\r\n",
      "Successfully installed en-core-web-sm-3.8.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ces9sNOgzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:46:43.934326Z",
     "start_time": "2025-11-26T18:46:43.928418Z"
    }
   },
   "source": [
    "def lemmatize_texts(texts,\n",
    "                    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'],\n",
    "                    forbidden_postags=[],\n",
    "                    as_sentence=False,\n",
    "                    get_postags=False,\n",
    "                    spacy_model=None):\n",
    "    \"\"\"Lemmatize a list of texts.\n",
    "\n",
    "            Please refer to https://spacy.io/api/annotation for details on the allowed\n",
    "        POS tags.\n",
    "        @params:\n",
    "            - texts_in: a list of texts, where each texts is a string\n",
    "            - allowed_postags: a list of part of speech tags, in the spacy fashion\n",
    "            - as_sentence: a boolean indicating whether the output should be a list of sentences instead of a list of word lists\n",
    "        @return:\n",
    "            - A list of texts where each entry is a list of words list or a list of sentences\n",
    "        \"\"\"\n",
    "    if spacy_model is None:\n",
    "        spacy_model = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    lemmatized_texts = []\n",
    "    all_postags = [] if get_postags else None\n",
    "    for text in texts:\n",
    "        if isinstance(text, list):\n",
    "            text = ' '.join(text)\n",
    "        doc = spacy_model(text)\n",
    "        lemmas = []\n",
    "        postags = [] if get_postags else None\n",
    "        for token in doc:\n",
    "            if not token.is_alpha:\n",
    "                continue\n",
    "            if allowed_postags and token.pos_ not in allowed_postags:\n",
    "                continue\n",
    "            if forbidden_postags and token.pos_ in forbidden_postags:\n",
    "                continue\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "            if get_postags:\n",
    "                postags.append(token.pos_)\n",
    "        if as_sentence:\n",
    "            lemmatized_texts.append(' '.join(lemmas))\n",
    "        else:\n",
    "            lemmatized_texts.append(lemmas)\n",
    "\n",
    "        if get_postags:\n",
    "            all_postags.append(postags)\n",
    "\n",
    "    if get_postags:\n",
    "        return lemmatized_texts, all_postags\n",
    "\n",
    "    return lemmatized_texts\n"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YGCUjGKmzb2x",
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T18:46:47.893320Z",
     "start_time": "2025-11-26T18:46:46.515019Z"
    }
   },
   "source": [
    "lemmatize_texts(words)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mass', 'shooting', 'week', 'tv'],\n",
       " ['join', 'official', 'song'],\n",
       " ['marry', 'time', 'age'],\n",
       " ['blast', 'artwork'],\n",
       " ['margulie', 'poop', 'bag', 'pick', 'dog'],\n",
       " ['devastate', 'sexual', 'harassment', 'claim', 'undermine', 'legacy'],\n",
       " ['tonight', 'bit'],\n",
       " ['watch', 'amazon', 'prime', 'week'],\n",
       " ['reveal', 'fourth', 'austin', 'power', 'film'],\n",
       " ['watch', 'week'],\n",
       " ['visit', 'school', 'shooting', 'victim'],\n",
       " ['south', 'korean', 'president', 'meet', 'summit'],\n",
       " ['life', 'risk', 'remote', 'oyster', 'grow', 'region', 'call', 'robot'],\n",
       " ['trump', 'crackdown', 'immigrant', 'parent', 'put', 'kid', 'strain'],\n",
       " ['son', 'concern', 'obtain', 'wiretap', 'ally', 'meet', 'jr'],\n",
       " ['trump', 'love'],\n",
       " ['hilariously', 'troll', 'trump', 'spy', 'claim'],\n",
       " ['vote', 'repeal', 'abortion', 'amendment', 'landslide', 'referendum'],\n",
       " ['critic', 'grand', 'pivot', 'conservation'],\n",
       " ['trump',\n",
       "  'scottish',\n",
       "  'golf',\n",
       "  'resort',\n",
       "  'pay',\n",
       "  'woman',\n",
       "  'significantly',\n",
       "  'man',\n",
       "  'report'],\n",
       " ['gift'],\n",
       " ['twitter', 'universally', 'entertain'],\n",
       " ['mystery',\n",
       "  'animal',\n",
       "  'reportedly',\n",
       "  'shoot',\n",
       "  'baffle',\n",
       "  'wildlife',\n",
       "  'official'],\n",
       " ['open', 'talk', 'trump', 'cancel', 'summit'],\n",
       " ['man', 'detonate', 'bomb', 'authority'],\n",
       " ['thousand', 'travel', 'home', 'vote', 'abortion', 'access'],\n",
       " ['irish',\n",
       "  'voter',\n",
       "  'set',\n",
       "  'liberalize',\n",
       "  'abortion',\n",
       "  'law',\n",
       "  'landslide',\n",
       "  'exit',\n",
       "  'poll',\n",
       "  'signal'],\n",
       " ['warrior', 'coach', 'call', 'ban', 'protest', 'fake', 'patriotism'],\n",
       " ['historic', 'victory', 'barbado', 'elect', 'female', 'prime', 'minister'],\n",
       " ['police', 'kill', 'black', 'moment', 'protest'],\n",
       " ['pardon', 'sex', 'criminalize'],\n",
       " ['step'],\n",
       " ['face', 'felony', 'charge', 'welfare', 'fraud'],\n",
       " ['join'],\n",
       " ['spend'],\n",
       " ['drop', 'marketing', 'campaign', 'harassment', 'accusation'],\n",
       " ['chinese', 'act', 'understand', 'immigration', 'politic', 'today'],\n",
       " ['trump', 'maga', 'theme', 'sink', 'twitter'],\n",
       " ['funny', 'regret', 'trump', 'summit'],\n",
       " ['turn', 'subway', 'rid', 'terrible', 'punishment'],\n",
       " ['man',\n",
       "  'face',\n",
       "  'charge',\n",
       "  'pull',\n",
       "  'knife',\n",
       "  'gun',\n",
       "  'muslim',\n",
       "  'student',\n",
       "  'mcdonald'],\n",
       " ['leader', 'watch'],\n",
       " ['people', 'injure', 'school', 'shooting'],\n",
       " ['people', 'rattle', 'nigerian'],\n",
       " ['prove', 'terrible', 'apologize'],\n",
       " ['creator', 'sue', 'maker', 'upcoming', 'adult', 'theme', 'puppet', 'film'],\n",
       " ['miss', 'saddest', 'death', 'avenger', 'infinity', 'war'],\n",
       " ['walk', 'story', 'drug', 'sexual', 'assault'],\n",
       " ['basically', 'war', 'zone'],\n",
       " ['ruin', 'romance'],\n",
       " ['pull'],\n",
       " ['final', 'game', 'throne', 'scene'],\n",
       " ['difficult', 'man', 'make', 'great', 'tv'],\n",
       " ['harvey', 'accuser', 'think', 'arrest'],\n",
       " ['harvey', 'weinstein', 'turn', 'police', 'allege', 'sex', 'crime'],\n",
       " ['budget', 'video', 'meme', 'worthy'],\n",
       " ['war', 'movie'],\n",
       " ['trump', 'suggest', 'summit', 'happen'],\n",
       " ['press', 'damaging', 'email', 'wikileak', 'report'],\n",
       " ['publix', 'suspend', 'contribution', 'nra', 'back', 'politician', 'protest'],\n",
       " ['bot', 'deliver', 'victory', 'research', 'paper'],\n",
       " ['bag', 'wet', 'sock', 'apparently', 'live', 'press', 'briefing', 'room'],\n",
       " ['member', 'proud', 'boy', 'menace', 'user', 'doorstep'],\n",
       " ['thing', 'nra', 'blame', 'school', 'shooting'],\n",
       " ['win', 'save'],\n",
       " ['trump', 'executive', 'order', 'make', 'easy', 'federal', 'worker'],\n",
       " ['set', 'merge', 'care'],\n",
       " ['joy', 'watch', 'walk'],\n",
       " ['funniest', 'tweet', 'woman', 'week'],\n",
       " ['star', 'give', 'desk', 'makeover', 'incredible'],\n",
       " ['old', 'gay', 'man', 'gay', 'hilarious', 'video'],\n",
       " ['race', 'teach', 'straight', 'people', 'queer', 'culture'],\n",
       " ['fish', 'market', 'buy', 'freedom'],\n",
       " ['threaten', 'trump', 'summit', 'warn', 'nuclear', 'showdown'],\n",
       " ['investigation', 'find', 'russian', 'missile'],\n",
       " ['demolish', 'tunnel', 'nuclear', 'test', 'site', 'report'],\n",
       " ['bait', 'trump', 'cancel', 'summit'],\n",
       " ['reveal', 'cover', 'upcoming', 'memoir'],\n",
       " ['late', 'night', 'writer', 'royal', 'wedding', 'recap'],\n",
       " ['accuse', 'inappropriate', 'behavior', 'sexual', 'harassment'],\n",
       " ['win', 'fine', 'player', 'anthem', 'protest'],\n",
       " ['drama'],\n",
       " ['infinity', 'war', 'writer', 'clear', 'marvel', 'problematic', 'timeline'],\n",
       " ['chew', 'let', 'cheek'],\n",
       " ['poehl', 'park', 'recreation', 'revival', 'amazing'],\n",
       " ['expose', 'big', 'flaw', 'trump', 'late', 'conspiracy', 'theory'],\n",
       " ['seth', 'meyer', 'valuable', 'marketing', 'advice'],\n",
       " ['launch', 'auto', 'import', 'probe', 'vow', 'defend', 'interest'],\n",
       " ['woman', 'pull', 'stop', 'land', 'dream', 'job'],\n",
       " ['wife',\n",
       "  'ask',\n",
       "  'husband',\n",
       "  'fenty',\n",
       "  'highlighter',\n",
       "  'pack',\n",
       "  'highlighter',\n",
       "  'hairbrush'],\n",
       " ['trump',\n",
       "  'roasted',\n",
       "  'rush',\n",
       "  'defend',\n",
       "  'tomi',\n",
       "  'lahren',\n",
       "  'ignore',\n",
       "  'real',\n",
       "  'victim'],\n",
       " ['abruptly', 'cancel', 'summit'],\n",
       " ['nune', 'link', 'winery', 'face', 'suit', 'allege', 'prostitute', 'party'],\n",
       " ['immigrant',\n",
       "  'child',\n",
       "  'accuse',\n",
       "  'border',\n",
       "  'patrol',\n",
       "  'abuse',\n",
       "  'neglect',\n",
       "  'report',\n",
       "  'show'],\n",
       " ['ireland',\n",
       "  'historic',\n",
       "  'vote',\n",
       "  'legalize',\n",
       "  'abortion',\n",
       "  'haunt',\n",
       "  'trump',\n",
       "  'brexit'],\n",
       " ['trump', 'lawyer', 'attend', 'meeting', 'confidential', 'informant'],\n",
       " ['advance', 'pick', 'plan', 'parenthood', 'kill', 'year'],\n",
       " ['win', 'interview', 'mueller', 'spygate', 'report'],\n",
       " ['running', 'judge', 'die', 'day', 'primary', 'win'],\n",
       " ['chrissy', 'teigen', 'taunt', 'twitter', 'block', 'ruling'],\n",
       " ['trump', 'favorite', 'congressman'],\n",
       " ['trump', 'posthumously', 'pardon'],\n",
       " ['donate', 'wildlife', 'fund'],\n",
       " ['reveal', 'tattoo', 'honor', 'manchester', 'attack'],\n",
       " ['wish', 'happy', 'anniversary', 'fine', 'pretty', 'cute'],\n",
       " ['dance', 'wreck', 'ball', 'kid', 'living', 'room'],\n",
       " ['farrow', 'defend', 'woody', 'allen', 'accuse', 'abuse'],\n",
       " ['threaten', 'kill', 'harvey', 'weinstein'],\n",
       " ['reveal', 'block', 'twitter'],\n",
       " ['address', 'verbal', 'harassment'],\n",
       " ['discuss', 'tambor', 'allege', 'harassment', 'male', 'star', 'brush'],\n",
       " ['weinstein', 'expect', 'turn', 'nypd', 'sex', 'crime'],\n",
       " ['torch', 'shut'],\n",
       " ['introduce',\n",
       "  'anti',\n",
       "  'abortion',\n",
       "  'bill',\n",
       "  'give',\n",
       "  'man',\n",
       "  'property',\n",
       "  'right',\n",
       "  'fetus'],\n",
       " ['critic', 'mercilessly', 'mock', 'trump', 'canceling', 'summit'],\n",
       " ['civil',\n",
       "  'right',\n",
       "  'group',\n",
       "  'rip',\n",
       "  'revise',\n",
       "  'sexual',\n",
       "  'harassment',\n",
       "  'policy'],\n",
       " ['sue', 'exclude', 'undocumented', 'immigrant', 'census', 'count'],\n",
       " ['vow', 'fight', 'predictably', 'lose', 'party', 'endorsement'],\n",
       " ['official', 'reportedly', 'consider', 'ignore', 'climate', 'research'],\n",
       " ['read', 'trump', 'letter', 'cancel', 'summit'],\n",
       " ['respond', 'trump', 'sudden', 'cancellation'],\n",
       " ['facebook',\n",
       "  'ad',\n",
       "  'rule',\n",
       "  'prove',\n",
       "  'basically',\n",
       "  'charge',\n",
       "  'regulating',\n",
       "  'election'],\n",
       " ['border', 'agent', 'fatally', 'shoot', 'migrant', 'woman'],\n",
       " ['trump', 'open', 'door', 'gun', 'industry', 'sell', 'firearm', 'abroad'],\n",
       " ['stormy', 'day', 'declare', 'star', 'key', 'city'],\n",
       " ['checkout', 'machine'],\n",
       " ['way', 'make', 'family', 'road', 'trip', 'easy', 'parent', 've'],\n",
       " ['shred', 'long', 'list', 'conspiracy', 'theory'],\n",
       " ['kneeling', 'ban'],\n",
       " ['rule', 'immigrant', 'hearing'],\n",
       " ['offer', 'internship', 'student', 'affect', 'gun', 'violence'],\n",
       " ['notaro', 'tell', 'ellen', 'degenere', 'son', 'passion', 'trash'],\n",
       " ['stormy',\n",
       "  'daniels',\n",
       "  'wonderful',\n",
       "  'gay',\n",
       "  'dad',\n",
       "  'west',\n",
       "  'hollywood',\n",
       "  'hand',\n",
       "  'city'],\n",
       " ['queer', 'eye', 'season', 'drop', 'month'],\n",
       " ['gay', 'man', 'deny', 'marriage', 'license', 'lose', 'bid', 'unseat'],\n",
       " ['dancing', 'bounce', 'baby', 'cute', 'thing'],\n",
       " ['player', 'unwilling', 'stand', 'anthem', 'shouldn', 'country'],\n",
       " ['facebook', 'accuse', 'read', 'text', 'access', 'microphone', 'lawsuit'],\n",
       " ['drive', 'uber', 'fatal', 'accident', 'second', 'react', 'crash'],\n",
       " ['admit', 'alexa', 'device', 'eavesdrop', 'family'],\n",
       " ['man', 'discover', 'handgun', 'impale', 'bumper', 'car'],\n",
       " ['rescue',\n",
       "  'cat',\n",
       "  'hilarious',\n",
       "  'wide',\n",
       "  'eyed',\n",
       "  'expression',\n",
       "  'confirm',\n",
       "  'pregnant'],\n",
       " ['weird', 'promposal', 'include', 'teen', 'fake', 'birth'],\n",
       " [],\n",
       " ['anti', 'establishment', 'party', 'set', 'form', 'government'],\n",
       " ['silverman', 'see', 'reason', 'continue', 'artist'],\n",
       " ['southern',\n",
       "  'baptist',\n",
       "  'leader',\n",
       "  'abuse',\n",
       "  'woman',\n",
       "  'pray',\n",
       "  'removed',\n",
       "  'post'],\n",
       " ['support', 'sex', 'marriage'],\n",
       " ['young', 'winner', 'voice'],\n",
       " ['stir', 'school', 'ice', 'undocumented', 'kid'],\n",
       " ['sandy', 'hook', 'family'],\n",
       " ['high', 'school', 'teacher', 'defeat', 'leader', 'primary'],\n",
       " ['make', 'history', 'win', 'democratic', 'nod', 'governor'],\n",
       " ['report', 'business', 'partner', 'cooperate', 'prosecutor'],\n",
       " ['totally', 'agree', 'hall', 'plaque'],\n",
       " ['scientist', 'turn', 'technology', 'search', 'loch', 'ness', 'monster'],\n",
       " ['late', 'night', 'writer', 'vow', 'red', 'hat', 'trump', 'supporter'],\n",
       " ['receive', 'american', 'humor'],\n",
       " ['share', 'intimate', 'wedding', 'photo', 'tribute', 'husband', 'barack'],\n",
       " ['spot', 'trevor', 'host', 'cameo', 'reveal'],\n",
       " ['glover', 'fan', 'execute', 'stunning', 'coup', 'theme', 'reddit', 'forum'],\n",
       " ['call',\n",
       "  'restaurant',\n",
       "  'duty',\n",
       "  'cop',\n",
       "  'allegedly',\n",
       "  'assault',\n",
       "  'black',\n",
       "  'woman'],\n",
       " ['rwandan', 'country', 'female', 'neurosurgeon'],\n",
       " ['win', 'democratic', 'primary', 'georgia', 'governor'],\n",
       " ['trevor', 'reveal', 'question', 'dare'],\n",
       " ['reflect', 'scary', 'time', 'snap'],\n",
       " ['reason', 'start', 'conversation', 'responsible', 'finishing'],\n",
       " ['lourde', 'vibe', 'moody', 'ad', 'campaign', 'pic'],\n",
       " ['way', 'trick', 'watch', 'show', 'movie'],\n",
       " ['pitt', 'demean', 'thing', 'exchange', 'deadpool', 'cameo'],\n",
       " ['trump', 'restaurant'],\n",
       " ['shut', 'fan', 'blame', 'split'],\n",
       " ['parent', 'slam', 'dog', 'scene', 'groom', 'child', 'sexual', 'abuse'],\n",
       " ['apologize', 'world', 'massive', 'jerk', 'trump'],\n",
       " ['introduce', 'pretty', 'embarrassing'],\n",
       " ['take', 'impression', 'level', 'tonight'],\n",
       " ['ready', 'party'],\n",
       " ['win', 'congressional', 'race'],\n",
       " ['fighter', 'pilot', 'win', 'democratic', 'primary'],\n",
       " ['trump', 'rally', 'conservative', 'promise', 'abortion', 'curb'],\n",
       " ['aclu', 'sue', 'strike', 'congressional', 'map'],\n",
       " ['late', 'school', 'shooting', 'hasn', 'stir', 'gun', 'debate'],\n",
       " ['tweet', 'rescue', 'season', 'finale'],\n",
       " ['grill', 'selfie'],\n",
       " ['netflix'],\n",
       " ['charge', 'church', 'leader', 'abuse', 'risk', 'teen', 'girl'],\n",
       " ['ban', 'kneel', 'national', 'anthem'],\n",
       " ['royal', 'wedding', 'spoof', 'ad'],\n",
       " ['report', 'progress', 'immigration', 'deal'],\n",
       " ['invite', 'review', 'classify', 'document', 'informant'],\n",
       " ['candidate', 'point', 'gun', 'teenager', 'advance', 'primary', 'runoff'],\n",
       " ['walk',\n",
       "  'definition',\n",
       "  'entitle',\n",
       "  'millennial',\n",
       "  'swear',\n",
       "  'entitle',\n",
       "  'millennial'],\n",
       " ['push', 'include', 'classified', 'briefing', 'informant'],\n",
       " ['machine', 'job', 'increasingly', 'rob', 'work', 'humanity'],\n",
       " ['trump', 'praise', 'restaurant', 'crowd', 'toss', 'water', 'insult'],\n",
       " ['sander', 'bother', 'call', 'liar'],\n",
       " ['trump', 'block', 'people', 'judge', 'rule'],\n",
       " ['win',\n",
       "  'iraqi',\n",
       "  'christian',\n",
       "  'parliamentary',\n",
       "  'seat',\n",
       "  'christian',\n",
       "  'trump',\n",
       "  'intervene'],\n",
       " ['tulsi', 'gabbard', 'view', 'cost', 'support', 'teacher', 'union'],\n",
       " ['receive', 'security', 'clearance', 'report'],\n",
       " ['congressional', 'candidate', 'compare', 'abortion', 'holocaust'],\n",
       " ['gun', 'reform', 'advocate', 'head', 'seat'],\n",
       " ['war', 'burlesque', 'storm', 'stormtrooper'],\n",
       " ['establishment', 'back', 'moderate', 'win', 'heat', 'democratic'],\n",
       " ['senator', 'unveil', 'crack', 'sexual', 'harassment'],\n",
       " ['long', 'weekend'],\n",
       " ['reveal', 'open', 'season', 'star', 'war', 'theme', 'park', 'land'],\n",
       " ['late', 'entry', 'comedy', 'wildlife', 'photo', 'award', 'cheer'],\n",
       " ['bad', 'lip', 'reading', 'reveal', 'royal', 'wedding'],\n",
       " ['australian', 'archbishop', 'convict', 'cover', 'child', 'sex', 'abuse'],\n",
       " ['commander', 'people', 'respond', 'pompeo', 'threat', 'punch', 'mouth'],\n",
       " ['extend', 'facebook', 'apology', 'tour', 'european', 'gig'],\n",
       " ['tweet', 'message', 'support', 'manchester', 'bombing', 'anniversary'],\n",
       " ['single', 'american', 'idol'],\n",
       " ['cop', 'watch', 'powerful', 'monologue', 'racism'],\n",
       " ['amazon',\n",
       "  'sell',\n",
       "  'cop',\n",
       "  'facial',\n",
       "  'recognition',\n",
       "  'tool',\n",
       "  'bad',\n",
       "  'potentially',\n",
       "  'racist'],\n",
       " ['wreck', 'ball', 'prank', 'sleep'],\n",
       " ['bayer', 'fear', 'porn', 'star', 'skit', 'snl'],\n",
       " ['rippon', 'win', 'dancing', 'star', 'destine'],\n",
       " ['imitate', 'chewbacca'],\n",
       " ['win', 'reveal', 'date'],\n",
       " ['resist', 'turn', 'trump', 'nickname', 'joke'],\n",
       " ['meyer', 'force', 'tweak', 'late', 'twitter', 'rant', 'time'],\n",
       " ['boy', 'teacher', 'tell', 'fault', 'police', 'shoot', 'age'],\n",
       " ['cop', 'accuse', 'racism', 'detain', 'black', 'man', 'vegetation'],\n",
       " ['white', 'cop', 'shout', 'black', 'suspect', 'pretend', 'shoot'],\n",
       " ['police', 'report', 'dead', 'apartment', 'complex'],\n",
       " ['board', 'pansexuality', 'solo'],\n",
       " ['charlize', 'play'],\n",
       " ['boycott', 'publix', 'store', 'donation', 'nra', 'back', 'candidate'],\n",
       " ['tell', 'stop', 'arrest', 'people', 'smoking', 'pot', 'street'],\n",
       " ['head', 'unaware', 'interference', 'boost', 'trump'],\n",
       " ['lawmaker', 'introduce', 'crowdfunde', 'pay', 'trump', 'border', 'wall'],\n",
       " ['lawmaker', 'allow', 'review', 'classify', 'info', 'investigation'],\n",
       " ['reveal', 'bad', 'royal', 'wedding', 'ceremony'],\n",
       " ['grande', 'reportedly', 'date', 'snl'],\n",
       " ['allegiance', 'student', 'bullet', 'riddled', 'school'],\n",
       " ['open', 'date', 'young', 'woman'],\n",
       " ['ceasefire', 'shred', 'sean', 'hannity', 'dummy', 'fox', 'friend'],\n",
       " ['royal', 'wedding', 'video', 'see', 'ceremony', 'take', 'turn'],\n",
       " ['primary', 'focus', 'gun', 'immigration'],\n",
       " ['movie', 'report', 'find', 'troubling', 'drop', 'lgbtq', 'representation'],\n",
       " ['offer', 'store', 'beauty', 'class', 'community'],\n",
       " ['primary', 'highlight', 'compete', 'vision', 'democratic', 'party'],\n",
       " ['move', 'school', 'worker', 'red', 'flag', 'dangerous', 'gun', 'owner'],\n",
       " ['talk', 'grievance', 'health', 'care', 'track'],\n",
       " ['problem', 'commemorative', 'trump'],\n",
       " ['stance', 'russian', 'collusion', 'illegal'],\n",
       " ['onion', 'hilariously', 'troll', 'cease', 'desist', 'note'],\n",
       " ['boost', 'school', 'funding', 'address', 'teacher', 'walkout'],\n",
       " ['email', 'lawmaker', 'shrug', 'trump', 'lax', 'cellphone', 'security'],\n",
       " ['stink', 'bug', 'flavor', 'jelly'],\n",
       " ['power',\n",
       "  'outage',\n",
       "  'trigger',\n",
       "  'extreme',\n",
       "  'zombie',\n",
       "  'activity',\n",
       "  'alert',\n",
       "  'city'],\n",
       " ['bank', 'post', 'record', 'profit'],\n",
       " [],\n",
       " ['reporter', 'shove', 'bar', 'meeting', 'water', 'contaminant'],\n",
       " ['russian', 'connection', 'aeroflot'],\n",
       " ['lonely', 'planet', 'european', 'destination', 'aim', 'overtourism'],\n",
       " ['insist', 'anti', 'spanish', 'rant', 'person'],\n",
       " ['reporter', 'reveal', 'trump', 'chilling', 'reason', 'slam', 'press'],\n",
       " ['reportedly', 'reject', 'phone', 'security', 'measure', 'inconvenient'],\n",
       " ['federal', 'court', 'side', 'transgender', 'teen', 'bathroom', 'fight'],\n",
       " ['trump',\n",
       "  'adviser',\n",
       "  'call',\n",
       "  'president',\n",
       "  'claim',\n",
       "  'informant',\n",
       "  'spied',\n",
       "  'campaign',\n",
       "  'embarrassing'],\n",
       " ['test', 'abandon', 'climate', 'diplomacy'],\n",
       " ['trump', 'successfully', 'drain', 'swamp'],\n",
       " ['hewitt', 'suggest', 'trench', 'gun', 'control'],\n",
       " ['bring', 'world'],\n",
       " ['paulson', 'unapologetic', 'year', 'age', 'gap', 'girlfriend'],\n",
       " ['side', 'gun', 'debate', 'agree', 'safe', 'storage', 'support'],\n",
       " ['family', 'gay', 'man', 'critically', 'injure', 'route', 'answer'],\n",
       " ['trump', 'bad', 'right'],\n",
       " ['boy',\n",
       "  'scout',\n",
       "  'allegedly',\n",
       "  'boot',\n",
       "  'store',\n",
       "  'company',\n",
       "  'win',\n",
       "  'support',\n",
       "  'homo'],\n",
       " ['add', 'mute', 'button', 'sake', 'sanity'],\n",
       " ['publix', 'censor', 'graduation', 'cake'],\n",
       " ['sexist', 'passenger', 'female', 'pilot', 'perfect', 'response'],\n",
       " ['time', 'take', 'aim', 'sexual', 'harassment', 'complaint'],\n",
       " ['socialist', 'leader', 'elect', 'allegation', 'irregularity'],\n",
       " ['support', 'nuclear', 'deal'],\n",
       " ['mob', 'attack', 'year', 'liberal', 'greek', 'mayor'],\n",
       " ['mende', 'perform', 'parkland', 'survivor', 'billboard', 'music', 'award'],\n",
       " ['rice', 'mother', 'open', 'youth', 'center', 'son', 'memory'],\n",
       " ['order', 'make', 'voter', 'registration', 'easy', 'update', 'dmv', 'info'],\n",
       " ['call', 'moment', 'honor', 'shooting', 'victim', 'billboard', 'award'],\n",
       " ['stop', 'rapping', 'word', 'onstage'],\n",
       " ['gay', 'man', 'tell', 'love'],\n",
       " ['unusual', 'asteroid', 'interstellar', 'guest', 'solar'],\n",
       " ['start', 'suicide', 'prevention', 'site', 'age', 'strong'],\n",
       " ['prepare', 'conjured', 'mueller', 'indictment'],\n",
       " ['election'],\n",
       " ['lot', 'hard', 'sue', 'employer'],\n",
       " ['hire', 'illegally', 'lobbyist', 'job'],\n",
       " ['explore', 'experience', 'late', 'book'],\n",
       " ['game', 'show', 'big', 'hurdle', 'lgbtq', 'athlete', 'exist', 'field'],\n",
       " ['chrissy', 'teigen', 'real', 'life', 'childbirth'],\n",
       " ['suit', 'creator', 'pen', 'open', 'letter', 'attend', 'royal', 'wedding'],\n",
       " ['movie', 'come', 'netflix', 'week'],\n",
       " ['come', 'queue'],\n",
       " ['make', 'theroux', 'giant', 'rat', 'tattoo'],\n",
       " ['tip', 'russian', 'trump', 'yale', 'commencement', 'speech'],\n",
       " ['talk', 'trump', 'trump', 'hear', 'source'],\n",
       " ['plan', 'big', 'anti', 'corruption', 'theme', 'midterm'],\n",
       " ['reject', 'moment', 'silence', 'call', 'change', 'billboard', 'award'],\n",
       " ['ask', 'american', 'public', 'settle', 'internet', 'dumb', 'debate'],\n",
       " ['viewer'],\n",
       " ['trump', 'team', 'move', 'lift', 'ban', 'extreme', 'hunting', 'tactic'],\n",
       " ['tell', 'trump', 'sexualized', 'ivanka', 'make', 'cringe'],\n",
       " ['help', 'company', 'sexual', 'harassment'],\n",
       " ['steal', 'spot', 'box', 'office', 'avenger', 'infinity', 'war'],\n",
       " ['westworld', 'subtly', 'tease', 'baby', 'theory'],\n",
       " ['tell', 'war', 'moment', 'son'],\n",
       " ['marvel', 'exec', 'stop', 'jar', 'infinity', 'war', 'death'],\n",
       " ['villain', 'spider', 'man', 'sequel'],\n",
       " ['receive'],\n",
       " ['video', 'capture', 'driver', 'racist', 'rant'],\n",
       " ['lady', 'unwitte', 'godmother', 'reality', 'tv', 'archetype'],\n",
       " ['year', 'boy', 'homeless', 'population'],\n",
       " ['mock', 'royal', 'wedding'],\n",
       " ['dangerously', 'unregulate', 'rehab', 'industry'],\n",
       " ['honor', 'journalist', 'year'],\n",
       " ['french', 'homophobia'],\n",
       " ['belgian', 'ballet', 'drama', 'win', 'big', 'canne', 'film', 'festival'],\n",
       " ['year', 'gay', 'man', 'prove', 'attend', 'pride'],\n",
       " ['meddle', 'receive'],\n",
       " ['airline',\n",
       "  'passenger',\n",
       "  'arrest',\n",
       "  'allegedly',\n",
       "  'harass',\n",
       "  'woman',\n",
       "  'peeing',\n",
       "  'seat'],\n",
       " ['promise', 'crush', 'achieve', 'deal'],\n",
       " ['tourist'],\n",
       " ['confirm', 'kill', 'country', 'deadly', 'plane', 'crash'],\n",
       " ['put', 'trade', 'war', 'hold'],\n",
       " ['trump', 'economic', 'adviser', 'amazon', 'threat', 'lane'],\n",
       " ['dem', 'trump', 'admin', 'common', 'ground', 'help', 'victim', 'genocide'],\n",
       " ['obstruction', 'probe', 'report'],\n",
       " ['german',\n",
       "  'broadcaster',\n",
       "  'criticize',\n",
       "  'racist',\n",
       "  'coverage',\n",
       "  'royal',\n",
       "  'wedding'],\n",
       " ['bold', 'teen', 'tv', 'ready', 'grow'],\n",
       " ['kelton', 'big', 'idea'],\n",
       " ['cancel', 'shotgun', 'school', 'shooting'],\n",
       " ['lead', 'snl', 'chorus', 'lament', 'trump'],\n",
       " ['day', 'gush', 'snl', 'smart', 'aleck', 'royal', 'wedding'],\n",
       " ['starbuck', 'sit', 'cafe', 'buying'],\n",
       " ['trump', 'face', 'ominous'],\n",
       " ['fiery', 'speech', 'close', 'canne', 'rap', 'harvey', 'weinstein'],\n",
       " ['snl'],\n",
       " ['parent', 'school', 'shooting', 'victim', 'decry', 'platitude'],\n",
       " ['trump', 'real', 'tough', 'probe'],\n",
       " ['chrissy', 'teigen', 'announce', 'baby', 'boy', 'adorable', 'photo'],\n",
       " ['tout', 'arm', 'teacher'],\n",
       " ['call', 'medium', 'stop', 'name', 'school', 'shooter'],\n",
       " ['trump', 'set', 'showdown', 'claim', 'campaign', 'surveillance'],\n",
       " ['late', 'painting', 'slam', 'kent', 'state', 'goldilock'],\n",
       " ['blame', 'school', 'shooting', 'ritalin'],\n",
       " ['trick', 'royal', 'fan', 'review', 'wedding', 'big', 'day'],\n",
       " ['fallon', 'reveal', 'viewer', 'big', 'wedding', 'fail'],\n",
       " ['protester', 'throw', 'razz', 'lawyer', 'rant', 'spanish', 'speaker'],\n",
       " ['watch', 'amazon', 'prime', 'week'],\n",
       " ['ferrell', 'molly', 'cover', 'royal', 'wedding', 'cord', 'tish'],\n",
       " ['artist', 'cedric', 'arrive'],\n",
       " ['trump', 'roll', 'nra', 'gun', 'parkland'],\n",
       " ['candidate', 'set', 'trust', 'pay', 'kid', 'marry', 'white', 'people'],\n",
       " ['chief', 'sick', 'inaction', 'gun', 'control'],\n",
       " ['history', 'national', 'anthem', 'sport'],\n",
       " ['troll', 'trump', 'bigly', 'huge', 'royal', 'wedding', 'crowd'],\n",
       " ['victim', 'high', 'school', 'shooting'],\n",
       " ['ceo', 'pay', 'cut', 'employee', 'year', 'battle', 'amazon'],\n",
       " ['meet', 'emissary', 'prince', 'offer', 'campaign', 'report'],\n",
       " ['roar'],\n",
       " ['shred', 'cowardly', 'trump', 'lawmaker', 'school', 'shooting'],\n",
       " ['reveal', 'thing', 'don'],\n",
       " ['watch', 'week'],\n",
       " ['student', 'stage', 'sit', 'gun', 'control', 'arrest'],\n",
       " ['know', 'scary', 'appearance', 'gate', 'daughter'],\n",
       " ['russian', 'spy', 'discharge', 'hospital', 'poisoning', 'incident'],\n",
       " ['plane', 'crash', 'reportedly', 'kill'],\n",
       " ['trump', 'reassure', 'threaten', 'total', 'decimation', 'deal'],\n",
       " ['fan', 'sing', 'time', 'surprise', 'live'],\n",
       " ['privacy', 'row'],\n",
       " ['high',\n",
       "  'schooler',\n",
       "  'hold',\n",
       "  'gun',\n",
       "  'violence',\n",
       "  'protest',\n",
       "  'month',\n",
       "  'shooting'],\n",
       " ['trailer', 'close'],\n",
       " ['power', 'immigration', 'court', 'limit', 'judge', 'authority'],\n",
       " ['introduce', 'demand', 'white', 'apologize', 'mccain'],\n",
       " ['graphic', 'describe', 'deep', 'mueller', 'probe'],\n",
       " ['wife', 'file', 'divorce'],\n",
       " ['meek', 'back', 'trump', 'event', 'prison', 'reform'],\n",
       " ['trump', 'rule', 'cut', 'federal', 'fund', 'clinic', 'provide', 'abortion'],\n",
       " ['trump', 'prison', 'reform', 'head'],\n",
       " ['single', 'chilean', 'offer', 'country', 'sex', 'abuse', 'scandal'],\n",
       " ['call', 'manchester', 'attack', 'bad', 'humanity', 'year'],\n",
       " ['chrissy', 'teigen', 'spill', 'detail', 'baby', 'boy', 'love'],\n",
       " ['baby', 'boy'],\n",
       " ['break', 'impression'],\n",
       " ['involve', 'sex', 'cult', 'place'],\n",
       " ['kill', 'yanny', 'laurel', 'debate', 'ridiculous', 'clip'],\n",
       " ['lady'],\n",
       " ['book', 'shadow', 'charm', 'trailer'],\n",
       " ['violinist', 'accuse', 'sexual', 'misconduct'],\n",
       " ['people', 'single', 'book'],\n",
       " ['ellen', 'degenere', 'hilariously', 'explain', 'save', 'life'],\n",
       " ['lawyer',\n",
       "  'rant',\n",
       "  'spanish',\n",
       "  'speaker',\n",
       "  'face',\n",
       "  'office',\n",
       "  'eviction',\n",
       "  'disciplinary',\n",
       "  'complaint'],\n",
       " ['hannity', 'probe', 'direct', 'threat', 'republic'],\n",
       " ['answer', 'global', 'health', 'security', 'head', 'sudden', 'departure'],\n",
       " ['gun', 'owner', 'gun', 'owner', 'agree', 'lot', 'gun', 'reform'],\n",
       " ['user', 'pan', 'school', 'shooting', 'victim'],\n",
       " ['ban', 'bump', 'stock', 'enforce', 'law'],\n",
       " ['liberal', 'billionaire', 'base'],\n",
       " ['heller', 'campaign', 'pay', 'social', 'medium', 'influencer', 'son'],\n",
       " ['interior', 'dept', 'dodge', 'watchdog', 'question', 'zinke', 'senator'],\n",
       " ['tease'],\n",
       " ['happytime', 'murder', 'adult', 'theme', 'movie', 'puppet'],\n",
       " ['hold', 'gun', 'crazy', 'slipper', 'magical', 'meme'],\n",
       " ['trump', 'didn', 'difference'],\n",
       " ['bio', 'terrible'],\n",
       " ['admit', 'hit', 'daughter', 'famous', 'pal', 'watch'],\n",
       " ['hospice', 'overdose', 'patient', 'hasten', 'death', 'admit'],\n",
       " ['bring', 'house', 'giving', 'trump', 'late', 'tweet', 'end'],\n",
       " ['wwf', 'wrestler', 'severely', 'beat', 'home'],\n",
       " ['pick', 'surprise', 'announcement'],\n",
       " ['trump', 'domestic', 'gag', 'rule', 'strip', 'fund', 'plan', 'parenthood'],\n",
       " ['reverse', 'trump', 'team', 'position'],\n",
       " ['anxiety', 'doesn', 'attention', 'deserve'],\n",
       " ['box', 'office', 'record'],\n",
       " ['obamacare', 'premium', 'high', 'year'],\n",
       " ['comprehensive', 'list', 'infamous', 'meeting'],\n",
       " ['cut', 'plea', 'deal', 'cooperate', 'government'],\n",
       " ['briefly', 'respond', 'school', 'shooting', 'pivot'],\n",
       " ['late', 'night', 'writer', 'shred', 'racist'],\n",
       " ['chief', 'apparently', 'change', 'tune', 'climate', 'change'],\n",
       " ['freedom', 'caucus', 'sink', 'key', 'reform', 'agenda'],\n",
       " ['transphobic',\n",
       "  'congressional',\n",
       "  'hopeful',\n",
       "  'berate',\n",
       "  'person',\n",
       "  'public',\n",
       "  'restroom'],\n",
       " ['singer', 'come'],\n",
       " ['new', 'head', 'cracker', 'hear', 'needle'],\n",
       " ['meditation', 'human', 'clearing', 'browser', 'cache'],\n",
       " ['funniest', 'tweet', 'woman', 'week'],\n",
       " ['populist', 'party', 'form', 'government'],\n",
       " ['business',\n",
       "  'group',\n",
       "  'quietly',\n",
       "  'kill',\n",
       "  'bring',\n",
       "  'sexual',\n",
       "  'abuse',\n",
       "  'claim',\n",
       "  'light'],\n",
       " ['family', 'reportedly', 'close', 'bailout', 'flagship', 'tower'],\n",
       " ['charity', 'royal', 'wedding'],\n",
       " ['federal',\n",
       "  'prosecutor',\n",
       "  'turn',\n",
       "  'trump',\n",
       "  'inauguration',\n",
       "  'protester',\n",
       "  'felon'],\n",
       " ['immigration', 'fight', 'derail', 'food', 'stamp'],\n",
       " ['retire', 'lawmaker', 'add', 'campaign', 'treasury', 'election'],\n",
       " ['face', 'farm', 'vote', 'problem', 'leader', 'make', 'deal'],\n",
       " ['methodist', 'minister', 'certificate'],\n",
       " ['mark', 'space', 'superpower'],\n",
       " ['trump', 'special', 'anniversary', 'gift'],\n",
       " ['trump',\n",
       "  'stormy',\n",
       "  'daniels',\n",
       "  'disclosure',\n",
       "  'leave',\n",
       "  'question',\n",
       "  'payment',\n",
       "  'cohen'],\n",
       " ['enjoy', 'animate', 'snl', 'return'],\n",
       " ['controversial', 'school', 'handle', 'student', 'misbehavior'],\n",
       " ['deaf', 'activist', 'point', 'marvel', 'diversity', 'problem', 'race'],\n",
       " ['magnum', 'trailer', 'lot', 'close', 'shave', 'big', 'mustache'],\n",
       " ['confirm', 'director', 'aid', 'democrat'],\n",
       " ['unanimously',\n",
       "  'vote',\n",
       "  'ban',\n",
       "  'assault',\n",
       "  'weapon',\n",
       "  'high',\n",
       "  'capacity',\n",
       "  'magazine'],\n",
       " ['chrissy', 'teigen', 'boy'],\n",
       " ['terrify', 'clip', 'show', 'run', 'tree', 'thunderstorm'],\n",
       " ['tap', 'bigot', 'religious', 'freedom', 'panel'],\n",
       " ['seth', 'meyer', 'unexpected', 'advice'],\n",
       " ['nune',\n",
       "  'fundraise',\n",
       "  'attack',\n",
       "  'tell',\n",
       "  'potential',\n",
       "  'donor',\n",
       "  'trump',\n",
       "  'hero'],\n",
       " ['mexican', 'warn', 've'],\n",
       " ['female', 'ceo', 'name', 'sexual', 'harassment', 'complaint'],\n",
       " ['battle', 'save', 'die', 'soil'],\n",
       " ['man', 'insult', 'starbuck', 'barista', 'write'],\n",
       " ['member', 'claim', 'trump', 'fake', 'bizarre', 'phone', 'rehearsal'],\n",
       " ['owe', 'apology'],\n",
       " ['trevor', 'fake', 'profit', 'university', 'ad'],\n",
       " ['explain'],\n",
       " ['war', 'writer', 'confirm', 'glover', 'character', 'pansexual', 'solo'],\n",
       " ['medical',\n",
       "  'professional',\n",
       "  'fact',\n",
       "  'check',\n",
       "  'grey',\n",
       "  'anatomy',\n",
       "  'sex',\n",
       "  'scene'],\n",
       " ['schumer', 'nail', 'fairytale', 'wedding', 'suck'],\n",
       " ['end', 'kent', 'state'],\n",
       " ['black', 'profile', 'share', 'story'],\n",
       " ['black', 'actress', 'protest', 'racism', 'french', 'film', 'industry'],\n",
       " ['deny', 'rumor', 'date', 'black', 'woman'],\n",
       " ['pac',\n",
       "  'rip',\n",
       "  'conservative',\n",
       "  'pro',\n",
       "  'police',\n",
       "  'anti',\n",
       "  'abortion',\n",
       "  'message',\n",
       "  'prosecutor'],\n",
       " ['mtv',\n",
       "  'suspend',\n",
       "  'catfish',\n",
       "  'probe',\n",
       "  'schulman',\n",
       "  'sexual',\n",
       "  'misconduct',\n",
       "  'claim'],\n",
       " ['trump',\n",
       "  'ice',\n",
       "  'increasingly',\n",
       "  'arrest',\n",
       "  'immigrant',\n",
       "  'criminal',\n",
       "  'conviction'],\n",
       " ['special', 'counsel', 'hit', 'medium', 'advisor', 'subpoena'],\n",
       " ['sexually', 'abuse', 'dee', 'canne', 'documentary'],\n",
       " ['oswalt', 'theory', 'change', 'watch', 'dark', 'knight'],\n",
       " ['holt', 'american', 'captive', 'beg', 'prison', 'video'],\n",
       " ['photographer', 'taunt', 'call', 'immigrant', 'animal'],\n",
       " ['backstreet', 'boy', 'release', 'song', 'breaking', 'heart'],\n",
       " ['put', 'gay', 'inclusive', 'twist', 'traditional', 'fairy', 'tale'],\n",
       " ['gay',\n",
       "  'conversion',\n",
       "  'therapy',\n",
       "  'survivor',\n",
       "  'recall',\n",
       "  'month',\n",
       "  'powerful',\n",
       "  'video'],\n",
       " ['expertly',\n",
       "  'spoofs',\n",
       "  'vanity',\n",
       "  'fair',\n",
       "  'annual',\n",
       "  'comedy',\n",
       "  'issue',\n",
       "  'cover'],\n",
       " ['offer', 'head', 'spin', 'defense', 'trump'],\n",
       " ['fix', 'subway', 'ad', 'shre', 'cuomo', 'mta', 'commuter', 'rejoice'],\n",
       " ['farm', 'huge', 'blow', 'animal'],\n",
       " ['email',\n",
       "  'interior',\n",
       "  'expect',\n",
       "  'learn',\n",
       "  'public',\n",
       "  'input',\n",
       "  'bear',\n",
       "  'ear',\n",
       "  'review'],\n",
       " ['whistleblower', 'leak', 'financial', 'potential', 'cover', 'report'],\n",
       " ['kidnapper', 'accuse', 'alligator', 'hold', 'man', 'captive'],\n",
       " ['head', 'woman', 'email', 'scandal'],\n",
       " ['threaten', 'reconsider', 'trump', 'summit', 'cancel', 'talk'],\n",
       " ['chemical', 'weapon', 'watchdog', 'find'],\n",
       " ['protest', 'killing', 'border'],\n",
       " ['researcher', 'uncover', 'secret', 'page'],\n",
       " ['piscotty', 'return', 'bereavement', 'hit', 'mom', 'tribute', 'home', 'run'],\n",
       " ['back', 'haspel', 'lead'],\n",
       " ['congressional', 'delegation', 'long', 'man'],\n",
       " ['war', 'spinoff', 'star', 'glover', 'happen'],\n",
       " ['fake', 'sean', 'hannity', 'pillow', 'talk', 'late'],\n",
       " ['rapper', 'arrest', 'disorderly', 'conduct', 'public', 'drunkenness'],\n",
       " ['conservative', 'cook', 'farm', 'gambit', 'vote', 'hardline', 'immigration'],\n",
       " ['novartis', 'lawyer', 'resign', 'deal'],\n",
       " ['birth'],\n",
       " ['eliminate', 'post', 'appointee', 'leaf'],\n",
       " ['emily',\n",
       "  'list',\n",
       "  'back',\n",
       "  'lawyer',\n",
       "  'win',\n",
       "  'democratic',\n",
       "  'primary',\n",
       "  'district'],\n",
       " ['tell', 'sweet', 'story', 'comic', 'store', 'visit'],\n",
       " ['attack', 'race'],\n",
       " ['empire', 'military', 'take', 'gate'],\n",
       " ['longtime',\n",
       "  'trump',\n",
       "  'lawyer',\n",
       "  'slam',\n",
       "  'polarizing',\n",
       "  'figure',\n",
       "  'deal',\n",
       "  'mueller'],\n",
       " ['award', 'nomination', 'screaming'],\n",
       " ['cancel', 'organizer', 'detain', 'authority'],\n",
       " ['attorney', 'vow', 'stop', 'prosecute', 'minor', 'case'],\n",
       " ['victim', 'receive', 'state', 'settlement'],\n",
       " ['leave',\n",
       "  'open',\n",
       "  'possibility',\n",
       "  'dad',\n",
       "  'know',\n",
       "  'trump',\n",
       "  'tower',\n",
       "  'meeting',\n",
       "  'time'],\n",
       " ['break', 'scene', 'laugh', 'mission', 'impossible', 'trailer'],\n",
       " ['live'],\n",
       " ['approve', 'restore', 'voting', 'right', 'people', 'probation', 'parole'],\n",
       " ['dress', 'celebration', 'black', 'star', 'war', 'character'],\n",
       " ['trump', 'pay', 'financial', 'disclosure', 'confirm'],\n",
       " ['american', 'free', 'jail'],\n",
       " ['settle', 'group', 'discrimination', 'lawsuit'],\n",
       " ['tweeter', 'hard', 'pass', 'rnc', 'dinner', 'photo', 'prize'],\n",
       " ['facebook', 'remove', 'fake', 'account', 'quarter'],\n",
       " ['pink', 'badass', 'response', 'troll', 'look'],\n",
       " ['adopt', 'software', 'automatically', 'clear', 'conviction'],\n",
       " ['young', 'return', 'broadway', 'root'],\n",
       " ['hot', 'seat', 'pruitt', 'dodge', 'question', 'mount', 'scandal'],\n",
       " ['inch', 'close', 'elect', 'nation', 'native', 'american', 'governor'],\n",
       " ['pro', 'abortion', 'right', 'progressive', 'win', 'primary'],\n",
       " ['crash', 'deadpool', 'crack', 'rat', 'trump', 'joke'],\n",
       " ['genuinely', 'hurt', 'trading', 'playground', 'insult'],\n",
       " [],\n",
       " ['warn', 'senator'],\n",
       " ['ban', 'single', 'plastic', 'straw'],\n",
       " ['grease', 'lightning', 'canne', 'dance', 'floor'],\n",
       " ['panel', 'release', 'detail', 'trump', 'tower', 'meeting', 'probe'],\n",
       " ['report',\n",
       "  'contradict',\n",
       "  'governor',\n",
       "  'candidate',\n",
       "  'test',\n",
       "  'animal',\n",
       "  'treatment'],\n",
       " ['republican', 'welfare', 'plan', 'keep', 'wage'],\n",
       " ['year', 'ago', 'today', 'moonwalke', 'tv', 'time'],\n",
       " ['publicly', 'drag', 'night'],\n",
       " ['artwork'],\n",
       " ['trump', 'failure', 'report', 'stormy', 'payoff', 'refer', 'prosecutor'],\n",
       " ['bohemian', 'rhapsody', 'trailer', 'slam', 'ignore', 'mercury', 'sexuality'],\n",
       " [],\n",
       " ['respond', 'multiple', 'assault', 'claim'],\n",
       " ['trump', 'refer', 'immigrant', 'animal'],\n",
       " ['angry',\n",
       "  'white',\n",
       "  'dude',\n",
       "  'rant',\n",
       "  'people',\n",
       "  'speak',\n",
       "  'spanish',\n",
       "  'nyc',\n",
       "  'viral'],\n",
       " ['democratic', 'lawmaker', 'pruitt', 'brutal', 'reality', 'check', 'hearing'],\n",
       " ['pick', 'hartogensis', 'lead', 'pension', 'agency'],\n",
       " ['search',\n",
       "  'leak',\n",
       "  'suspect',\n",
       "  'turn',\n",
       "  'photo',\n",
       "  'sex',\n",
       "  'assault',\n",
       "  'unconscious',\n",
       "  'friend'],\n",
       " ['pick', 'census', 'role', 'citizenship', 'question', 'political'],\n",
       " ['agitated', 'infuriate', 'meeting', 'focus', 'adoption'],\n",
       " ['sander', 'unusually', 'honest', 'seth', 'meyer', 'spoof', 'briefing'],\n",
       " [],\n",
       " ['raptor', 'flight'],\n",
       " ['surprise', 'show', 'come', 'netflix'],\n",
       " ['warn', 'grad', 'live', 'alternative', 'reality'],\n",
       " ['force', 'vote', 'net', 'neutrality', 'rule'],\n",
       " ['blame', 'victim', 'victimize', 'newly', 'release', 'video'],\n",
       " ['video', 'show', 'year', 'autism', 'taunt', 'abuse', 'bus', 'driver'],\n",
       " ['yanny', 'laurel', 'real', 'answer'],\n",
       " ['populist', 'cleric', 'win', 'parliamentary', 'election'],\n",
       " ['family',\n",
       "  'month',\n",
       "  'die',\n",
       "  'israeli',\n",
       "  'tear',\n",
       "  'protest',\n",
       "  'crackdown',\n",
       "  'graphic'],\n",
       " ['pilot', 'suck', 'halfway', 'break', 'windshield'],\n",
       " ['retail', 'group', 'ad', 'bash', 'tariff'],\n",
       " ['state', 'cover', 'postage', 'mail', 'ballot', 'year'],\n",
       " ['people', 'civil', 'politic', 'opponent', 'disagree'],\n",
       " ['investigate', 'report'],\n",
       " ['adorable', 'video', 'kid', 'show', 'activism', 'literally', 'easy', 'pie'],\n",
       " ['blame', 'deadly', 'violence'],\n",
       " ['conduct', 'interrogation', 'program'],\n",
       " ['tesla',\n",
       "  'autopilot',\n",
       "  'engage',\n",
       "  'horrific',\n",
       "  'mph',\n",
       "  'crash',\n",
       "  'driver',\n",
       "  'tell',\n",
       "  'police'],\n",
       " ['sell', 'plane', 'save', 'life', 'exit', 'nuclear', 'deal', 'make', 'hard'],\n",
       " ['fall', 'officer', 'trump', 'tout', 'policy', 'rail', 'sanctuary', 'city'],\n",
       " ['team', 'push', 'inquiry', 'prosecutor', 'charge'],\n",
       " ['pledge', 'big', 'money', 'turn', 'heat', 'gerrymandering'],\n",
       " ['gay', 'conversion', 'therapy', 'maryland'],\n",
       " ['judicial', 'analyst', 'defend', 'timeline', 'mueller', 'probe'],\n",
       " ['ridicule', 'trump', 'hannity', 'bedtime', 'chat'],\n",
       " ['rhapsody'],\n",
       " ['meyer', 'shred', 'trump', 'scandal', 'drain', 'swamp', 'devoid', 'meaning'],\n",
       " ['blood', 'dripping', 'portrait', 'take', 'aim', 'national', 'nra'],\n",
       " ['day', 'document'],\n",
       " ['rabid', 'anti', 'muslim', 'agent', 'training', 'police'],\n",
       " ['explain', 'choose', 'true', 'daughter'],\n",
       " ['track', 'confirm', 'director'],\n",
       " ['hat', 'prepare', 'play'],\n",
       " ['deeply', 'nerdy', 'filming'],\n",
       " ['win', 'repay', 'sexual', 'harassment', 'settlement'],\n",
       " ['major', 'love', 'blackkklansman', 'canne', 'rip', 'trump'],\n",
       " ['user', 'hilariously', 'skewer', 'terrible', 'stock', 'photo', 'job'],\n",
       " ['jar', 'visit', 'slam'],\n",
       " ['rippon', 'honor', 'mom', 'stunning', 'dancing', 'star', 'performance'],\n",
       " ['hit', 'genealogist', 'research', 'family', 'miss', 'point'],\n",
       " ['candidate', 'immigrant', 'deportation', 'bus'],\n",
       " ['rip', 'ivanka', 'trump', 'embassy', 'opening'],\n",
       " ['death', 'custody', 'rule', 'homicide'],\n",
       " ['savage', 'peace', 'treaty', 'barbie', 'collusion', 'embassy', 'opening'],\n",
       " ['user', 'roast', 'expert', 'salary', 'save'],\n",
       " ['woman',\n",
       "  'challenge',\n",
       "  'white',\n",
       "  'male',\n",
       "  'dominate',\n",
       "  'tech',\n",
       "  'industry',\n",
       "  'inside'],\n",
       " ['richard', 'explain', 'rid', 'trump', 'year', 'ago'],\n",
       " ['damon', 'completely', 'clayne', 'twitter', 'outburst'],\n",
       " ['war', 'story', 'forget'],\n",
       " ['reveal', 'multiple', 'physical', 'assault', 'exit'],\n",
       " ['fallon', 'make', 'sweet', 'music', 'cardboard', 'instrument'],\n",
       " ['trevor', 'news'],\n",
       " ['pass',\n",
       "  'controversial',\n",
       "  'tax',\n",
       "  'city',\n",
       "  'big',\n",
       "  'company',\n",
       "  'combat',\n",
       "  'housing',\n",
       "  'crisis'],\n",
       " ['end',\n",
       "  'force',\n",
       "  'arbitration',\n",
       "  'individual',\n",
       "  'case',\n",
       "  'sexual',\n",
       "  'assault',\n",
       "  'harassment'],\n",
       " ['meet', 'openly', 'man', 'college'],\n",
       " ['cancel', 'renew', 'tv', 'show'],\n",
       " ['teen', 'claim', 'visit', 'death', 'experience'],\n",
       " ['tell', 'shocked', 'harm'],\n",
       " ['farming', 'technique', 'drastically', 'water', 'catch'],\n",
       " ['kristen',\n",
       "  'ditch',\n",
       "  'heel',\n",
       "  'canne',\n",
       "  'red',\n",
       "  'carpet',\n",
       "  'defy',\n",
       "  'flat',\n",
       "  'rule'],\n",
       " ['address', 'nutty', 'fan', 'theory', 'file'],\n",
       " ['overturn', 'law'],\n",
       " ['shy', 'terrorist', 'organization'],\n",
       " ['respond', 'girl', 'song', 'controversy', 'apologize', 'lgbtq', 'community'],\n",
       " ['emotional', 'support', 'ban'],\n",
       " ['people', 'agree', 'voice', 'yanny', 'laurel'],\n",
       " ['world', 'deep', 'ocean', 'trench', 'safe', 'plastic', 'bag'],\n",
       " ['open', 'embassy', 'nation', 'prefer', 'wait', 'peace', 'deal'],\n",
       " ['israeli', 'force', 'kill', 'dozen', 'protest', 'intensify', 'embassy'],\n",
       " ['suspend', 'app', 'datum', 'misuse', 'investigation'],\n",
       " ['diagnose', 'pancreatic', 'cancer'],\n",
       " ['federal', 'regulator', 'take', 'aim', 'facebook'],\n",
       " ['apology', 'die', 'mccain', 'dig'],\n",
       " ['south',\n",
       "  'dakota',\n",
       "  'congressional',\n",
       "  'candidate',\n",
       "  'terrorist',\n",
       "  'attack',\n",
       "  'win',\n",
       "  'primary'],\n",
       " ['love', 'woman', 'spinoff'],\n",
       " ['coffee',\n",
       "  'shop',\n",
       "  'refuse',\n",
       "  'serve',\n",
       "  'man',\n",
       "  'racist',\n",
       "  'rant',\n",
       "  'woman',\n",
       "  'niqab'],\n",
       " ['invalidate', 'ban', 'sport', 'gambling'],\n",
       " ['huge', 'congrat', 'guest', 'bai', 'book'],\n",
       " ['movie', 'come', 'netflix', 'week'],\n",
       " ['prosecutor', 'drop', 'felony', 'invasion', 'privacy', 'charge'],\n",
       " ['israeli', 'soccer', 'team', 'add', 'trump'],\n",
       " ['lawmaker',\n",
       "  'pass',\n",
       "  'gender',\n",
       "  'neutral',\n",
       "  'bathroom',\n",
       "  'send',\n",
       "  'powerful',\n",
       "  'message'],\n",
       " ['girl', 'deem', 'tone', 'deaf', 'harmful', 'lgbtq', 'pop', 'artist'],\n",
       " ['open', 'door', 'protest', 'rage', 'nearby'],\n",
       " ['reveal', 'secret', 'figure', 'trump', 'mind'],\n",
       " ['people', 'freak'],\n",
       " ['channe', 'tatum', 'mother', 'message', 'love'],\n",
       " ['sexually', 'harass', 'fan'],\n",
       " ['lane', 'pay', 'tribute', 'late', 'lead'],\n",
       " ['onion', 'story', 'blake', 'farenthold', 'job', 'lobby'],\n",
       " ['signing', 'country', 'strict', 'abortion', 'ban'],\n",
       " ['promise', 'great', 'gender', 'equality', 'time', 'activism'],\n",
       " ['hit', 'spot', 'week'],\n",
       " ['perform', 'standup', 'tour'],\n",
       " ['coach', 'tell', 'hurt', 'ufc', 'fighter', 'continue'],\n",
       " ['recover', 'kidney', 'surgery'],\n",
       " ['vulnerable', 'voting'],\n",
       " ['engage', 'mother', 'day'],\n",
       " ['dolph', 'barista', 'fire', 'playing', 'song', 'pay'],\n",
       " ['bed', 'explore', 'intimacy', 'experience', 'sexual', 'assault'],\n",
       " ['trump', 'change', 'burger', 'order', 'make', 'healthy'],\n",
       " ['photo', 'cohen', 'qatari', 'investor', 'accuse', 'bribery'],\n",
       " ['lash', 'call', 'leak', 'user', 'pounce'],\n",
       " ['trump',\n",
       "  'order',\n",
       "  'chinese',\n",
       "  'phone',\n",
       "  'maker',\n",
       "  'approve',\n",
       "  'money',\n",
       "  'trump',\n",
       "  'project'],\n",
       " ['hundred', 'protest', 'city', 'poor', 'people', 'campaign'],\n",
       " ['response', 'ask', 'attend', 'royal', 'wedding'],\n",
       " ['romance', 'good'],\n",
       " ['game', 'change', 'westworld', 'brain', 'capsule', 'reveal', 'begin'],\n",
       " ['actress', 'play', 'superman', 'dead'],\n",
       " ['own', 'genealogist', 'remark', 'low', 'skilled', 'immigrant'],\n",
       " ['crash', 'actor', 'interview', 'life'],\n",
       " ['deny'],\n",
       " ['replace', 'crawford', 'renew', 'lethal', 'weapon'],\n",
       " ['zach', 'wood', 'officially'],\n",
       " ['korean', 'singing', 'disguise', 'unicorn'],\n",
       " ['billie', 'honor', 'tough', 'mother', 'day', 'photo'],\n",
       " ['pink', 'ask', 'year', 'concertgoer', 'sing', 'blow'],\n",
       " ['pastor', 'think', 'speak', 'embassy', 'opening'],\n",
       " ['hit',\n",
       "  'datum',\n",
       "  'breach',\n",
       "  'credit',\n",
       "  'debit',\n",
       "  'card',\n",
       "  'information',\n",
       "  'compromise'],\n",
       " ['trump', 'call', 'disarm', 'shooter'],\n",
       " ['host', 'trump', 'fulfil', 'biblical', 'prophecy', 'move', 'embassy'],\n",
       " ['keyless', 'car', 'kill', 'dozen', 'people', 'report'],\n",
       " ['rare', 'conjoin', 'fawn', 'scientist', 'double'],\n",
       " ['kill', 'dozen', 'injure', 'church', 'bombing'],\n",
       " ['country', 'remain', 'nuclear', 'deal'],\n",
       " ['official', 'owe', 'mccain', 'apology', 'disgusting', 'insult'],\n",
       " ['schumer', 'girl', 'live'],\n",
       " ['make', 'deal', 'depend'],\n",
       " ['attend', 'nra', 'convention', 'fake', 'news', 'medium'],\n",
       " ['autopilot', 'probe', 'tesla', 'crash', 'mph'],\n",
       " ['slam', 'epidemic', 'political', 'lie', 'danger', 'democracy'],\n",
       " ['massive',\n",
       "  'cookout',\n",
       "  'throw',\n",
       "  'park',\n",
       "  'cop',\n",
       "  'call',\n",
       "  'black',\n",
       "  'family',\n",
       "  'bbq'],\n",
       " ['embarrasse', 'weekend', 'update'],\n",
       " ['handmaid',\n",
       "  'tale',\n",
       "  'meet',\n",
       "  'sex',\n",
       "  'city',\n",
       "  'funny',\n",
       "  'terrify',\n",
       "  'snl',\n",
       "  'spoof'],\n",
       " ['video',\n",
       "  'show',\n",
       "  'security',\n",
       "  'guard',\n",
       "  'choke',\n",
       "  'black',\n",
       "  'teen',\n",
       "  'accuse',\n",
       "  'shoplifting'],\n",
       " ['trump', 'praise', 'late', 'mom', 'mother', 'day'],\n",
       " ['trump', 'promise', 'chinese', 'company', 'violate', 'sanction'],\n",
       " ['sander', 'berate', 'staffer', 'mccain', 'leak'],\n",
       " ['parkland', 'dad', 'point', 'message', 'president'],\n",
       " ['sanction', 'company'],\n",
       " ['share', 'adorable', 'pic', 'touch', 'mother', 'day', 'tribute'],\n",
       " ['claim', 'low', 'skilled', 'immigrant', 'country', 'base'],\n",
       " ['stalk', 'cheetah', 'oblivious', 'family', 'scare', 'exit', 'car'],\n",
       " ['portray', 'trump', 'finger', 'flip'],\n",
       " ['italian', 'tribunal', 'lift', 'ban', 'silvio', 'hold', 'public', 'office'],\n",
       " ['plan', 'dismantle', 'nuclear', 'test', 'site', 'end'],\n",
       " ['attack', 'leave', 'dead', 'injure'],\n",
       " ['watch', 'week'],\n",
       " ['watch', 'amazon', 'prime', 'week'],\n",
       " ['unveil', 'spoof', 'line', 'trump', 'theme', 'mother', 'day', 'gift'],\n",
       " ['find', 'tv', 'home', 'star', 'fan', 'rejoice'],\n",
       " ['share', 'inspire', 'tale', 'fresh', 'prince', 'air'],\n",
       " ['group', 'law', 'discriminatory'],\n",
       " ['mueller',\n",
       "  'reportedly',\n",
       "  'investigate',\n",
       "  'foreign',\n",
       "  'link',\n",
       "  'donor',\n",
       "  'trump',\n",
       "  'inauguration',\n",
       "  'fund'],\n",
       " ['trump', 'pence', 'portrait', 'post', 'office'],\n",
       " ['dog', 'sale'],\n",
       " ['southern', 'baptist', 'leader', 'apologize', 'hurtful', 'comment', 'woman'],\n",
       " ['progressive',\n",
       "  'group',\n",
       "  'spend',\n",
       "  'million',\n",
       "  'elect',\n",
       "  'reformist',\n",
       "  'prosecutor'],\n",
       " ['incredible', 'photo', 'world', 'protect'],\n",
       " ['avenger',\n",
       "  'infinity',\n",
       "  'war',\n",
       "  'director',\n",
       "  'reveal',\n",
       "  'die',\n",
       "  'survive',\n",
       "  'screen'],\n",
       " ['uber', 'silence', 'woman', 'sexual', 'assault'],\n",
       " ['marvel', 'introduce', 'official', 'chinese', 'superhero'],\n",
       " ['big',\n",
       "  'bang',\n",
       "  'theory',\n",
       "  'pay',\n",
       "  'sweetest',\n",
       "  'tribute',\n",
       "  'hawk',\n",
       "  'delete',\n",
       "  'scene'],\n",
       " ['maher', 'shred', 'liken', 'mob', 'boss'],\n",
       " ['hold', 'hostage'],\n",
       " ['psycho', 'bite', 'portrait'],\n",
       " ['protester'],\n",
       " ['understand', 'straight', 'incel', 'talk', 'gay'],\n",
       " ['gather', 'military', 'spouse', 'put', 'white'],\n",
       " ['game',\n",
       "  'host',\n",
       "  'prematurely',\n",
       "  'congratulate',\n",
       "  'contestant',\n",
       "  'mortifying',\n",
       "  'fail'],\n",
       " ['red', 'carpet', 'protest', 'inequality', 'film'],\n",
       " ['man', 'charge', 'hate', 'crime', 'beat', 'gay', 'couple'],\n",
       " ['administration', 'roll', 'protection', 'prison', 'inmate'],\n",
       " ['reportedly', 'pay', 'time'],\n",
       " ['metoo', 'statehouse', 'man', 'harassment', 'culture'],\n",
       " ['bar', 'police', 'department', 'sell', 'gun', 'public'],\n",
       " ['masterclass',\n",
       "  'parody',\n",
       "  'dubbing',\n",
       "  'movie',\n",
       "  'swear',\n",
       "  'word',\n",
       "  'falcon',\n",
       "  'great'],\n",
       " ['make', 'easy', 'confirm', 'judge'],\n",
       " [],\n",
       " ['meyer', 'skewer', 'idea', 'win', 'peace'],\n",
       " ['turn', 'trump', 'campaign'],\n",
       " ['trailer', 'ax', 'tremor', 'offer', 'glimpse'],\n",
       " ['frighten', 'rabbit', 'singer', 'dead'],\n",
       " ['suspect', 'custody', 'shoot', 'high', 'school', 'injure'],\n",
       " ['name', 'official', 'job', 'unqualified', 'staffer'],\n",
       " ['trevor', 'bite', 'break', 'bad', 'themed', 'nickname'],\n",
       " ['moral', 'robbin', 'season', 'snatch'],\n",
       " ['pose', 'character', 'find', 'scene'],\n",
       " ['accuse',\n",
       "  'golden',\n",
       "  'state',\n",
       "  'killer',\n",
       "  'face',\n",
       "  'additional',\n",
       "  'murder',\n",
       "  'count'],\n",
       " ['roast', 'spoof', 'ceremony', 'video'],\n",
       " ['regret', 'perpetual', 'punch'],\n",
       " ['kill', 'bad', 'mass', 'shooting'],\n",
       " [],\n",
       " ['opposition', 'trump', 'hot', 'lawyer'],\n",
       " ['dog', 'prepare', 'baby', 'cat', 'prepare', 'teenager'],\n",
       " ['rebuke', 'official', 'mock', 'die'],\n",
       " ['drag', 'race', 'queen', 'turn', 'time', 'cher', 'rusical'],\n",
       " ['administration', 'axis', 'fund', 'monitor', 'greenhouse', 'gas'],\n",
       " ['police', 'search', 'suspect', 'serial', 'killer', 'dump', 'ground'],\n",
       " [],\n",
       " ['embody', 'fight', 'future', 'democratic', 'party'],\n",
       " ['hacker', 'election', 'night', 'report'],\n",
       " ['gay', 'teacher', 'suspend', 'show', 'photo', 'future', 'wife', 'sue'],\n",
       " ['school',\n",
       "  'district',\n",
       "  'allegedly',\n",
       "  'lgbtq',\n",
       "  'student',\n",
       "  'read',\n",
       "  'bible',\n",
       "  'punishment'],\n",
       " ['trump', 'launch', 'war', 'people'],\n",
       " ['fan', 'make', 'punk', 'music', 'debut', 'talk', 'girl', 'party'],\n",
       " ['retire', 'insult'],\n",
       " ['fox', 'cut', 'tie', 'cite', 'torture'],\n",
       " ['trump', 'attempt', 'make', 'immigrant', 'caravan', 'backfiring'],\n",
       " ['celebrity', 'read', 'weird', 'text', 'mom', 'mother', 'day', 'treat'],\n",
       " ['stormy', 'daniels', 'actress', 'doppelganger'],\n",
       " ['old', 'smoke', 'dozen', 'cigar', 'day'],\n",
       " ['tweeter', 'erupt', 'trump', 'aide', 'die'],\n",
       " ['wed', 'bang', 'theory', 'fan', 'geek'],\n",
       " ['hide', 'dinner', 'climate', 'denier', 'accuse', 'child', 'sex', 'abuse'],\n",
       " ['take', 'cancer'],\n",
       " ['talk', 'white', 'people', 'call', 'cop', 'people', 'color'],\n",
       " ['woman', 'call', 'police', 'black', 'family', 'bbqe'],\n",
       " ['stand', 'come', 'year', 'cancel'],\n",
       " [],\n",
       " ['warn', 'document', 'release'],\n",
       " ['friend',\n",
       "  'host',\n",
       "  'mock',\n",
       "  'isis',\n",
       "  'coverage',\n",
       "  'unaware',\n",
       "  'beat',\n",
       "  'fox',\n",
       "  'story'],\n",
       " ['congressional', 'candidate', 'nra', 'tv', 'ad'],\n",
       " ['tell', 'remove', 'find', 'canadian', 'politician'],\n",
       " ['show'],\n",
       " ['woman', 'text', 'love', 'excessive', 'thing'],\n",
       " ['extortion', 'guy', 'figure', 'stormy', 'lawyer', 'pretty'],\n",
       " ['porter', 'survive', 'domestic', 'abuse', 'campaign'],\n",
       " ['trump', 'plan', 'prescription', 'drug', 'price', 'promise'],\n",
       " ['speak', 'trump', 'big', 'dollar', 'client'],\n",
       " ['russian', 'troll', 'facebook', 'ad', 'barely', 'target', 'swing', 'state'],\n",
       " ['meet', 'group', 'call', 'apartheid', 'call', 'injustice'],\n",
       " ['plan', 'family', 'vacation', 'travel', 'expert'],\n",
       " ['dynamic', 'create', 'run', 'robot', 'come'],\n",
       " ['student',\n",
       "  'deliver',\n",
       "  'thesis',\n",
       "  'underwear',\n",
       "  'professor',\n",
       "  'question',\n",
       "  'outfit'],\n",
       " ['funniest', 'tweet', 'woman', 'week'],\n",
       " ['reportedly', 'fire', 'dozen', 'rocket'],\n",
       " ['launch', 'airstrike', 'iranian', 'attack'],\n",
       " ['die', 'advocate', 'end', 'life', 'listen', 'ode', 'joy'],\n",
       " ['hold', 'return'],\n",
       " ['dozen', 'dead', 'torrential', 'rain', 'dam', 'burst'],\n",
       " ['spotify',\n",
       "  'stop',\n",
       "  'promote',\n",
       "  'kelly',\n",
       "  'music',\n",
       "  'sexual',\n",
       "  'assault',\n",
       "  'allegation'],\n",
       " ['trump'],\n",
       " ['homeland',\n",
       "  'security',\n",
       "  'deny',\n",
       "  'report',\n",
       "  'secretary',\n",
       "  'draft',\n",
       "  'resignation',\n",
       "  'letter'],\n",
       " ['cecile', 'richard', 'run', 'office', 'future'],\n",
       " ['give', 'incorrect', 'information', 'voter', 'registration'],\n",
       " ['pose', 'profit', 'queer', 'group'],\n",
       " ['renew', 'whop', 'episode'],\n",
       " ['warn', 'deadpool', 'fan', 'don', 'king', 'word'],\n",
       " ['nurse', 'charge', 'death', 'trump', 'adviser', 'father'],\n",
       " ['expect', 'hit'],\n",
       " ['documentary'],\n",
       " ['face', 'challenge', 'lawmaker', 'pull', 'support', 'immigration'],\n",
       " ['urge'],\n",
       " ['month', 'rio', 'cop', 'supposedly', 'close'],\n",
       " ['poorly', 'dress', 'fan', 'style', 'advice'],\n",
       " ['woman', 'come', 'job'],\n",
       " ['man', 'plead', 'guilty', 'bacon'],\n",
       " ['wrap'],\n",
       " ['ready', 'old'],\n",
       " ['net', 'neutrality', 'rule', 'end'],\n",
       " ['democratic',\n",
       "  'candidate',\n",
       "  'maryland',\n",
       "  'governor',\n",
       "  'die',\n",
       "  'cardiac',\n",
       "  'arrest'],\n",
       " ['officer', 'put', 'man', 'chokehold'],\n",
       " ['trevor', 'nutty', 'concession', 'speech'],\n",
       " ['cancel', 'twitter', 'mourn'],\n",
       " ['quiet', 'struggle', 'leave'],\n",
       " ['utterly', 'unrecognizable', 'undercover', 'boss'],\n",
       " ['girl', 'accuse', 'plot', 'kill', 'classmate'],\n",
       " ['curious', 'case', 'russian', 'national', 'park', 'service', 'latrine'],\n",
       " ['watch', 'completely', 'torpedo', 'trump', 'argument'],\n",
       " ['business', 'guest', 'defend', 'torture', 'work'],\n",
       " ['eye', 'remarry'],\n",
       " ['pass', 'historic', 'transgender', 'right'],\n",
       " ['question', 'torture'],\n",
       " ['run', 'election'],\n",
       " ['block', 'defended'],\n",
       " ['stay', 'snl', 'long'],\n",
       " ['mom', 'problem', 'hilarious', 'video', 'ellen'],\n",
       " ['suggest', 'restart', 'torture', 'interrogation', 'program'],\n",
       " ['greta', 'gerwig', 'loudly', 'critique', 'feel', 'pretty', 'live', 'tweet'],\n",
       " ['meet', 'secret', 'probe', 'source', 'threaten', 'doj'],\n",
       " ['engage', 'director'],\n",
       " ['jet', 'launch', 'apple', 'store', 'incredible', 'deal'],\n",
       " ['hardy', 'completely', 'unrecognizable', 'capone', 'movie'],\n",
       " ['woman', 'allege', 'rap', 'trap', 'brown', 'home'],\n",
       " ['court', 'seat', 'deny', 'year'],\n",
       " ['man', 'want', 'white', 'supremacist', 'kill', 'black', 'neighbor', 'burn'],\n",
       " ['trump', 'announce', 'detail', 'meet'],\n",
       " ['deliver', 'blister'],\n",
       " ['trump', 'joker', 'nightmare'],\n",
       " ['renew', 'hostility', 'huckabee', 'sander'],\n",
       " ['fan', 'erupt', 'fate'],\n",
       " ['street', 'apologize', 'teacher', 'live'],\n",
       " ['star', 'math'],\n",
       " ['pollen', 'bomb', 'allergy', 'worst', 'nightmare'],\n",
       " ['lopez', 'dance', 'danceable', 'song', 'tonight'],\n",
       " ['student',\n",
       "  'call',\n",
       "  'cop',\n",
       "  'black',\n",
       "  'schoolmate',\n",
       "  'napping',\n",
       "  'dorm',\n",
       "  'common',\n",
       "  'room'],\n",
       " ['quit', 'law', 'firm', 'wild', 'week', 'interview'],\n",
       " ['make', 'public', 'appearance', 'come', 'transgender'],\n",
       " ['reportedly', 'break'],\n",
       " ['avenger', 'leak', 'support', 'year', 'interview', 'marvel', 'director'],\n",
       " ['racist', 'impact', 'proposal'],\n",
       " ['complaint', 'minute', 'interview', 'stormy', 'daniel'],\n",
       " ['jail', 'home', 'trump', 'situation', 'bad'],\n",
       " ['teen', 'comedy', 'turn', 'sex'],\n",
       " ['woman', 'home', 'kitten', 'bobcat'],\n",
       " ['video',\n",
       "  'bear',\n",
       "  'eat',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  'dairy',\n",
       "  'queen',\n",
       "  'lead',\n",
       "  'charge',\n",
       "  'zoo',\n",
       "  'owner'],\n",
       " ['iranian',\n",
       "  'lawmaker',\n",
       "  'mock',\n",
       "  'trump',\n",
       "  'mental',\n",
       "  'capacity',\n",
       "  'burn',\n",
       "  'flag',\n",
       "  'pull',\n",
       "  'deal'],\n",
       " ['scramble', 'deal', 'trump', 'withdraw'],\n",
       " ['release',\n",
       "  'american',\n",
       "  'political',\n",
       "  'prisoner',\n",
       "  'advance',\n",
       "  'historic',\n",
       "  'summit'],\n",
       " ['establishment', 'notch', 'win', 'primary', 'vacant', 'house', 'seat'],\n",
       " ['praise', 'movement', 'day', 'abuse', 'allegation', 'emerge'],\n",
       " ['trevor', 'unload', 'suppose', 'advocate', 'woman'],\n",
       " ['year',\n",
       "  'comey',\n",
       "  'fire',\n",
       "  'scope',\n",
       "  'mueller',\n",
       "  'probe',\n",
       "  'expand',\n",
       "  'lawyer',\n",
       "  'shell',\n",
       "  'company'],\n",
       " ['trump', 'inspire', 'businessman', 'win', 'primary'],\n",
       " ['team', 'terrorist', 'silence', 'trump', 'pick'],\n",
       " ['job', 'program', 'people', 'work'],\n",
       " ['parson', 'queer', 'story', 'onscreen'],\n",
       " ['trump', 'accuser', 'win', 'democratic', 'nomination', 'legislature'],\n",
       " ['mastermind', 'waterboarde', 'information', 'share'],\n",
       " ['conservative',\n",
       "  'back',\n",
       "  'trump',\n",
       "  'agenda',\n",
       "  'defeat',\n",
       "  'progressive',\n",
       "  'challenger'],\n",
       " ['swift', 've', 'live'],\n",
       " ['make', 'history', 'fashion', 'council', 'influencer', 'award'],\n",
       " ['believe', 'accuser', 'long', 'investigation'],\n",
       " ['ability', 'change', 'voter', 'datum', 'didn', 'report'],\n",
       " ['win', 'democratic', 'gubernatorial', 'primary'],\n",
       " ['leader', 'guilty', 'firing', 'gun', 'charlottesville', 'rally'],\n",
       " ['protester', 'decry', 'torture', 'drag', 'hearing'],\n",
       " ['hire', 'firm', 'link', 'scoop', 'trump'],\n",
       " ['selling', 'sicken', 'theme', 'merch', 'mother', 'day'],\n",
       " ['defeat', 'relief', 'family', 'upper', 'big', 'branch', 'miner'],\n",
       " ['colbert', 'shred', 'trump', 'broken', 'promise', 'talk'],\n",
       " ['offend', 'host', 'hour', 'day'],\n",
       " ['come', 'year'],\n",
       " ['backstreet', 'boy', 'dress', 'spice', 'girl', 'large', 'life'],\n",
       " ['meyer', 'decode', 'teen', 'reveal', 'mean'],\n",
       " ['memorial'],\n",
       " ['trevor', 'turn', 'knife', 'comment', 'reggae', 'magic'],\n",
       " ['senator',\n",
       "  'force',\n",
       "  'vote',\n",
       "  'ultimately',\n",
       "  'reverse',\n",
       "  'restore',\n",
       "  'net',\n",
       "  'neutrality'],\n",
       " ['plan', 'place', 'replacement'],\n",
       " ['ambassador', 'write', 'letter', 'oppose', 'haspel', 'nomination'],\n",
       " ['home', 'solar', 'power'],\n",
       " ['governor', 'assign', 'investigation', 'allegation'],\n",
       " ['refugee', 'border', 'crackdown'],\n",
       " ['late', 'state', 'fund', 'gun', 'violence', 'prevention'],\n",
       " ['son', 'catch', 'tape', 'berate', 'cop', 'faggot'],\n",
       " ['progressive', 'prosecutor', 'win', 'primary'],\n",
       " ['novartis', 'money', 'pay', 'trump', 'lawyer'],\n",
       " ['safety', 'booklet', 'point', 'page', 'page'],\n",
       " ['uninsured', 'rate', 'worsen', 'year', 'obamacare', 'gain'],\n",
       " ['sander', 'plan', 'revive', 'labor', 'union'],\n",
       " ['bestow', 'nickname', 'late', 'painting'],\n",
       " ['male', 'ally', 'abuser'],\n",
       " ['century', 'long', 'relationship', 'boy', 'scout'],\n",
       " ['snack', 'addiction', 'kill', 'orangutan'],\n",
       " ['pick', 'director', 'win', 'torture', 'program', 'morally', 'wrong'],\n",
       " ['burn', 'cash', 'claim'],\n",
       " ['pinata',\n",
       "  'maker',\n",
       "  'apologize',\n",
       "  'hang',\n",
       "  'black',\n",
       "  'figure',\n",
       "  'likeness',\n",
       "  'lynching'],\n",
       " ['caliphate', 'trump', 'planet', 'ruin'],\n",
       " ['gay', 'marriage', 'opponent', 'oust'],\n",
       " ['eliminate', 'public', 'editor', 'role', 'call', 'outdate'],\n",
       " ['sell', 'access', 'trump', 'group', 'pay', 'shell', 'company'],\n",
       " ['mock', 'big', 'cloud', 'cocaine'],\n",
       " ['lose', 'race'],\n",
       " ['voter', 'pass', 'gerrymandering', 'reform', 'measure'],\n",
       " ['trump', 'deal', 'exit', 'win'],\n",
       " ['vote', 'homo', 'cuomo'],\n",
       " ['issue', 'binary', 'birth', 'certificate'],\n",
       " ['child',\n",
       "  'sexual',\n",
       "  'predator',\n",
       "  'roman',\n",
       "  'polanski',\n",
       "  'call',\n",
       "  'movement',\n",
       "  'hypocrisy'],\n",
       " ['reveal', 'detail', 'childish', 'gambino', 'viral', 'video'],\n",
       " ['crew', 'black', 'man', 'aren', 'allow', 'victim', 'dead'],\n",
       " [],\n",
       " ['judicial', 'analyst', 'issue', 'dark', 'warning', 'trump'],\n",
       " ['black',\n",
       "  'woman',\n",
       "  'bishop',\n",
       "  'make',\n",
       "  'history',\n",
       "  'predominantly',\n",
       "  'white',\n",
       "  'church'],\n",
       " [],\n",
       " ['heart', 'wrench', 'farewell', 'ncis'],\n",
       " ['lethal', 'weapon', 'fire', 'clayne', 'crawford', 'character', 'shot'],\n",
       " ['gomez', 'poke', 'fun', 'meet'],\n",
       " ['protest', 'leader', 'pashinian', 'elect'],\n",
       " ['secretly', 'meet', 'jinping'],\n",
       " ['claim', 'perfect', 'audience', 'trump'],\n",
       " ['chef', 'blast', 'serve'],\n",
       " ['world', 'leader', 'condemn', 'trump', 'withdraw', 'deal'],\n",
       " ['shun'],\n",
       " ['continue', 'attorney', 'general', 'exit'],\n",
       " ['announce', 'deal'],\n",
       " ['politician', 'physical', 'abuse', 'allegation'],\n",
       " ['trump', 'legal', 'strategy', 'monkey', 'throw', 'poo', 'wall'],\n",
       " ['kirsten', 'dunst', 'plemon', 'baby', 'boy'],\n",
       " ['matrimony', 'proposal', 'meet', 'gala'],\n",
       " ['apologize', 'store', 'accuse', 'black', 'man', 'theft'],\n",
       " ['trump', 'lawyer', 'incriminate', 'information', 'president'],\n",
       " ['stun', 'meet', 'gala', 'queer', 'superhero'],\n",
       " ['accuser', 'permission', 'bathroom'],\n",
       " ['trump', 'reportedly', 'tell', 'macron', 'pull', 'deal'],\n",
       " ['deportation', 'crackdown', 'hurt', 'immigrant', 'victim', 'crime'],\n",
       " ['meyer', 'apologize', 'air', 'trump'],\n",
       " ['blessing', 'meet', 'gala', 'tweet'],\n",
       " ['farm', 'cut', 'food', 'stamp', 'friendly', 'farmer'],\n",
       " ['deal'],\n",
       " ['day', 'bet', 'win', 'thrill', 'woman'],\n",
       " ['democratic', 'meddling', 'thing', 'watch', 'election', 'day'],\n",
       " ['meet', 'gala', 'nail', 'feature', 'chapel', 'paint', 'black', 'woman'],\n",
       " ['come', 'ted', 'year'],\n",
       " ['solange', 'durag', 'true', 'mvp', 'meet', 'gala'],\n",
       " ['threaten', 'official', 'census', 'citizenship', 'request'],\n",
       " ['hard', 'make', 'sense', 'rudy', 'late', 'trump', 'defense'],\n",
       " ['religious', 'imagery', 'stitched', 'meet', 'gala'],\n",
       " ['cop', 'racially', 'profile', 'checking', 'airbnb'],\n",
       " ['call', 'anti', 'bullying', 'campaign', 'ignore', 'husband', 'action'],\n",
       " ['trevor', 'watch', 'smoking', 'weed', 'tv'],\n",
       " ['receive', 'award', 'award'],\n",
       " ['prepare', 'send', 'spending', 'cut', 'package'],\n",
       " ['back', 'federal', 'prison', 'reform'],\n",
       " ['accuse', 'stalk', 'detective', 'film'],\n",
       " ['passenger', 'lash', 'pruitt', 'justify', 'class', 'memo'],\n",
       " ['progressive', 'battleground'],\n",
       " ['win', 'primary', 'race', 'seat'],\n",
       " ['marine', 'investigate', 'avowed', 'white', 'supremacist', 'active', 'duty'],\n",
       " ['violate', 'nuclear', 'deal', 'vow', 'reimpose', 'sanction'],\n",
       " ['sexual', 'abuse', 'complaint', 'taekwondo', 'star', 'lawsuit'],\n",
       " ['bring', 'family', 'meet', 'gala', 'noticing'],\n",
       " ['reilly', 'offer', 'stand', 'sander', 'handle', 'line', 'reporter'],\n",
       " ['catholic',\n",
       "  'priest',\n",
       "  'praise',\n",
       "  'dress',\n",
       "  'dude',\n",
       "  'meet',\n",
       "  'gala',\n",
       "  'living',\n",
       "  'life'],\n",
       " ['hide', 'seek', 'genius', 'dog', 'heal', 'nation'],\n",
       " ['appear', 'flip', 'promise'],\n",
       " ['caller', 'present', 'icon', 've'],\n",
       " [],\n",
       " ['head', 'replace', 'rise'],\n",
       " ['tease', 'song', 'surprise', 'meet', 'gala', 'performance'],\n",
       " ['iron', 'man', 'black', 'widow'],\n",
       " ['accept', 'firm', 'tie', 'russian', 'oligarch', 'report'],\n",
       " ['hit', 'fact', 'check'],\n",
       " ['send', 'olive', 'branch', 'taylor', 'swift', 'world', 'rejoice'],\n",
       " ['marry'],\n",
       " ['spill', 'bean', 'major', 'cameo', 'movie'],\n",
       " ['shawn', 'mende', 'sweetly', 'attend', 'hailey', 'gown', 'meet', 'gala'],\n",
       " ['hatch', 'suggest', 'trump', 'allow'],\n",
       " ['meyer', 'perfect', 'line', 'confront', 'lie'],\n",
       " ['hope', 'killer'],\n",
       " ['lili', 'sprouse', 'finally', 'make', 'couple', 'debut', 'meet', 'gala'],\n",
       " ['sweetbitter', 'capture', 'grit', 'glamour', 'restaurant', 'world'],\n",
       " ['rule', 'confirm', 'trump', 'judge'],\n",
       " ['zinke', 'tale', 'fish'],\n",
       " ['resign', 'abuse', 'allegation'],\n",
       " ['sanctuary', 'city', 'work', 'trump', 'era', 'surprising', 'cost'],\n",
       " ['pull', 'deal', 'decision', 'violate'],\n",
       " ['stormy', 'daniels', 'find', 'crunchy', 'chip', 'trump'],\n",
       " ['mountain', 'seuss', 'book']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bvxpCeyzb2x"
   },
   "source": [
    "# Using pre-trained Word2Vec representations with spacy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VxRq0WrOzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:46:57.965537Z",
     "start_time": "2025-11-26T18:46:57.963206Z"
    }
   },
   "source": [
    "import spacy"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KP9d31POzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:46:59.101216Z",
     "start_time": "2025-11-26T18:46:58.938806Z"
    }
   },
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jIqMz1FAzb2x",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "e9ce74e1-b782-42fb-8197-3b751a112e1b",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:46:59.369350Z",
     "start_time": "2025-11-26T18:46:59.360941Z"
    }
   },
   "source": [
    "nlp(\"this is a course\")[3].vector"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42265153, -0.5098841 ,  0.44870305, -0.5285095 ,  0.11685106,\n",
       "       -0.25577748,  1.0440685 , -0.6234342 ,  0.52448034,  0.02670155,\n",
       "       -1.0449603 , -0.9965604 , -0.9318913 ,  0.85351706,  0.22150773,\n",
       "        1.0939845 , -0.38940096, -0.8385448 , -0.6037917 , -0.00938578,\n",
       "       -0.02278345,  1.4837512 , -0.03643414, -1.0040534 , -0.22405809,\n",
       "        0.15180145,  0.9614223 , -0.67766124,  2.0248334 ,  0.4884538 ,\n",
       "       -0.18538043, -0.14550008, -0.26777828,  0.09275165,  0.29580584,\n",
       "        0.0228235 , -0.14496118, -0.33055532, -0.04477099,  0.03898396,\n",
       "        0.08150554,  0.3985741 ,  0.4057638 , -0.3282049 , -0.13090251,\n",
       "       -0.26520854, -0.24339822, -0.2178362 ,  0.00204343,  0.69068575,\n",
       "        0.88966286, -0.7896048 ,  0.64216405, -0.06787793,  0.8719513 ,\n",
       "       -1.429285  ,  1.64572   ,  0.24707851, -0.03324599, -0.7541893 ,\n",
       "       -0.47137254, -0.07451518, -0.9631462 , -1.1130385 ,  0.27110606,\n",
       "       -0.42041785, -0.807752  ,  0.076277  , -0.12623101, -0.08898473,\n",
       "        0.21823415,  0.7268755 , -0.24646385,  0.6792572 , -0.56881154,\n",
       "       -0.27999982,  0.94996846, -0.48908317,  0.5900008 , -0.7168647 ,\n",
       "       -0.01815934, -0.35813576,  0.10152262,  0.4145263 , -0.85537696,\n",
       "        0.7435371 ,  0.03457595, -0.20187059,  0.54058826,  0.33481508,\n",
       "        0.58146155,  0.19126174,  1.2424572 , -0.2613311 , -0.6280726 ,\n",
       "        0.11987524], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "87Sk_Wabzb2x",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:47:01.354375Z",
     "start_time": "2025-11-26T18:47:01.349859Z"
    }
   },
   "source": [
    "def get_word_embeddings(texts, occurences):\n",
    "    \"\"\"Return the word embeddings of the words in the texts.\n",
    "\n",
    "        @param:\n",
    "            - texts: a list of texts, where each text is a list of words\n",
    "            - occurences: a pandas DataFrame containing the occurences of each word in the dataset\n",
    "        @return:\n",
    "            - A pandas DataFrame containing the words and their embeddings\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    embeddings_dict = {}\n",
    "    for word in occurences.index:\n",
    "        doc = nlp(word)\n",
    "        if len(doc) > 0:\n",
    "            vector = doc[0].vector\n",
    "            embeddings_dict[word] = vector\n",
    "\n",
    "    embedding_df = pd.DataFrame.from_dict(\n",
    "        embeddings_dict,\n",
    "        orient='index',\n",
    "        columns=[f'dim_{i}' for i in range(len(next(iter(embeddings_dict.values()))))]\n",
    "    )\n",
    "    embedding_df.index.name = 'word'\n",
    "    embedding_df = embedding_df.reset_index()\n",
    "    embedding_df = embedding_df.merge(\n",
    "        occurences.reset_index().rename(columns={'index': 'word', 0: 'count'}),\n",
    "        on='word',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    return embedding_df"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "source": [
    "embeding = get_word_embeddings(words, pd.Series(compute_word_occurences(words)))"
   ],
   "metadata": {
    "id": "IWFsDPm0C4Vj",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:47:09.134794Z",
     "start_time": "2025-11-26T18:47:03.765620Z"
    }
   },
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "source": [
    "embeding"
   ],
   "metadata": {
    "id": "fWh96mujDWkP",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:47:09.890373Z",
     "start_time": "2025-11-26T18:47:09.873202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           word     dim_0     dim_1     dim_2     dim_3     dim_4     dim_5  \\\n",
       "0          mass  0.244948 -0.977806  1.067856 -0.447499 -0.033698 -0.639320   \n",
       "1     shootings -0.142742  0.960732 -0.380880 -0.570895  0.924472 -1.247212   \n",
       "2         texas  0.165466 -0.559187  0.317080 -0.302047  1.359532 -0.730086   \n",
       "3          week -0.113421 -1.045401  0.217601 -0.571715 -0.230358 -0.385906   \n",
       "4            tv -0.605170 -0.615427  0.032946  0.083007 -0.572484 -0.425285   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "3458    rainbow -1.195446 -0.255102  0.546870  0.579395 -0.216036  0.191490   \n",
       "3459  mountains -0.444491  1.350498 -0.287688  0.262064  0.833819 -0.680500   \n",
       "3460       peru -0.985710 -0.828765  0.349747  0.492182  1.139879 -1.170674   \n",
       "3461         dr -0.792492 -1.243818  0.944725 -0.359393 -0.139831 -0.424244   \n",
       "3462      seuss  0.351036  0.141784 -0.324931 -0.464101 -0.225673 -0.261663   \n",
       "\n",
       "         dim_6     dim_7     dim_8  ...    dim_87    dim_88    dim_89  \\\n",
       "0    -0.366450  0.384890  0.706800  ...  0.464070 -1.212034  0.154104   \n",
       "1     1.620525  1.516497 -0.348302  ... -0.064074 -0.147015 -0.469718   \n",
       "2     0.566777  0.814771  0.216252  ... -0.180898 -1.093605  1.006367   \n",
       "3     1.916841 -0.043089  0.003744  ...  0.750035 -0.080313 -0.146165   \n",
       "4     0.336877  0.658442 -0.342036  ... -0.206791 -0.290390  0.103737   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3458  0.405340  0.762400 -0.224344  ... -1.033622 -0.774358  0.058254   \n",
       "3459  1.466376  1.574952 -0.296974  ...  0.631165 -0.442193 -0.621174   \n",
       "3460  0.773269  0.485399  0.089902  ...  0.453476 -0.810936 -0.243210   \n",
       "3461  0.061578  0.682457 -0.623598  ...  0.650828 -0.774872  0.994104   \n",
       "3462 -0.822348  0.883749 -0.537540  ...  0.925906 -0.332249  0.790110   \n",
       "\n",
       "        dim_90    dim_91    dim_92    dim_93    dim_94    dim_95  count  \n",
       "0    -0.184826 -0.141946  1.385892  0.337580 -0.352089  1.024782      2  \n",
       "1     0.060155  0.645325  0.829488 -0.193837 -0.873957 -0.450171      2  \n",
       "2    -0.268461  0.946722  0.408462 -0.336604  1.203013  0.494927     15  \n",
       "3    -0.935099  0.057151  0.603119  0.220809  0.209795  0.531792     14  \n",
       "4    -0.453485  0.806918 -0.473440  0.497502 -0.570024  0.672677      9  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "3458 -0.228005  0.733140 -0.269310  0.035472 -0.697249  0.157006      1  \n",
       "3459 -0.778822 -0.154124  1.763509 -0.456275 -0.382693 -0.580011      1  \n",
       "3460 -0.864821  0.359710  1.711864 -0.356597 -0.168307  0.628820      1  \n",
       "3461 -0.298543  0.031190 -0.035435 -0.358144  0.346977  1.121911      1  \n",
       "3462 -0.503354  0.752054  0.694911  0.176286 -0.718676  1.355223      1  \n",
       "\n",
       "[3463 rows x 98 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_87</th>\n",
       "      <th>dim_88</th>\n",
       "      <th>dim_89</th>\n",
       "      <th>dim_90</th>\n",
       "      <th>dim_91</th>\n",
       "      <th>dim_92</th>\n",
       "      <th>dim_93</th>\n",
       "      <th>dim_94</th>\n",
       "      <th>dim_95</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mass</td>\n",
       "      <td>0.244948</td>\n",
       "      <td>-0.977806</td>\n",
       "      <td>1.067856</td>\n",
       "      <td>-0.447499</td>\n",
       "      <td>-0.033698</td>\n",
       "      <td>-0.639320</td>\n",
       "      <td>-0.366450</td>\n",
       "      <td>0.384890</td>\n",
       "      <td>0.706800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464070</td>\n",
       "      <td>-1.212034</td>\n",
       "      <td>0.154104</td>\n",
       "      <td>-0.184826</td>\n",
       "      <td>-0.141946</td>\n",
       "      <td>1.385892</td>\n",
       "      <td>0.337580</td>\n",
       "      <td>-0.352089</td>\n",
       "      <td>1.024782</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shootings</td>\n",
       "      <td>-0.142742</td>\n",
       "      <td>0.960732</td>\n",
       "      <td>-0.380880</td>\n",
       "      <td>-0.570895</td>\n",
       "      <td>0.924472</td>\n",
       "      <td>-1.247212</td>\n",
       "      <td>1.620525</td>\n",
       "      <td>1.516497</td>\n",
       "      <td>-0.348302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064074</td>\n",
       "      <td>-0.147015</td>\n",
       "      <td>-0.469718</td>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.645325</td>\n",
       "      <td>0.829488</td>\n",
       "      <td>-0.193837</td>\n",
       "      <td>-0.873957</td>\n",
       "      <td>-0.450171</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texas</td>\n",
       "      <td>0.165466</td>\n",
       "      <td>-0.559187</td>\n",
       "      <td>0.317080</td>\n",
       "      <td>-0.302047</td>\n",
       "      <td>1.359532</td>\n",
       "      <td>-0.730086</td>\n",
       "      <td>0.566777</td>\n",
       "      <td>0.814771</td>\n",
       "      <td>0.216252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180898</td>\n",
       "      <td>-1.093605</td>\n",
       "      <td>1.006367</td>\n",
       "      <td>-0.268461</td>\n",
       "      <td>0.946722</td>\n",
       "      <td>0.408462</td>\n",
       "      <td>-0.336604</td>\n",
       "      <td>1.203013</td>\n",
       "      <td>0.494927</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>week</td>\n",
       "      <td>-0.113421</td>\n",
       "      <td>-1.045401</td>\n",
       "      <td>0.217601</td>\n",
       "      <td>-0.571715</td>\n",
       "      <td>-0.230358</td>\n",
       "      <td>-0.385906</td>\n",
       "      <td>1.916841</td>\n",
       "      <td>-0.043089</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750035</td>\n",
       "      <td>-0.080313</td>\n",
       "      <td>-0.146165</td>\n",
       "      <td>-0.935099</td>\n",
       "      <td>0.057151</td>\n",
       "      <td>0.603119</td>\n",
       "      <td>0.220809</td>\n",
       "      <td>0.209795</td>\n",
       "      <td>0.531792</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tv</td>\n",
       "      <td>-0.605170</td>\n",
       "      <td>-0.615427</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.083007</td>\n",
       "      <td>-0.572484</td>\n",
       "      <td>-0.425285</td>\n",
       "      <td>0.336877</td>\n",
       "      <td>0.658442</td>\n",
       "      <td>-0.342036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206791</td>\n",
       "      <td>-0.290390</td>\n",
       "      <td>0.103737</td>\n",
       "      <td>-0.453485</td>\n",
       "      <td>0.806918</td>\n",
       "      <td>-0.473440</td>\n",
       "      <td>0.497502</td>\n",
       "      <td>-0.570024</td>\n",
       "      <td>0.672677</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>rainbow</td>\n",
       "      <td>-1.195446</td>\n",
       "      <td>-0.255102</td>\n",
       "      <td>0.546870</td>\n",
       "      <td>0.579395</td>\n",
       "      <td>-0.216036</td>\n",
       "      <td>0.191490</td>\n",
       "      <td>0.405340</td>\n",
       "      <td>0.762400</td>\n",
       "      <td>-0.224344</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.033622</td>\n",
       "      <td>-0.774358</td>\n",
       "      <td>0.058254</td>\n",
       "      <td>-0.228005</td>\n",
       "      <td>0.733140</td>\n",
       "      <td>-0.269310</td>\n",
       "      <td>0.035472</td>\n",
       "      <td>-0.697249</td>\n",
       "      <td>0.157006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>mountains</td>\n",
       "      <td>-0.444491</td>\n",
       "      <td>1.350498</td>\n",
       "      <td>-0.287688</td>\n",
       "      <td>0.262064</td>\n",
       "      <td>0.833819</td>\n",
       "      <td>-0.680500</td>\n",
       "      <td>1.466376</td>\n",
       "      <td>1.574952</td>\n",
       "      <td>-0.296974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631165</td>\n",
       "      <td>-0.442193</td>\n",
       "      <td>-0.621174</td>\n",
       "      <td>-0.778822</td>\n",
       "      <td>-0.154124</td>\n",
       "      <td>1.763509</td>\n",
       "      <td>-0.456275</td>\n",
       "      <td>-0.382693</td>\n",
       "      <td>-0.580011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>peru</td>\n",
       "      <td>-0.985710</td>\n",
       "      <td>-0.828765</td>\n",
       "      <td>0.349747</td>\n",
       "      <td>0.492182</td>\n",
       "      <td>1.139879</td>\n",
       "      <td>-1.170674</td>\n",
       "      <td>0.773269</td>\n",
       "      <td>0.485399</td>\n",
       "      <td>0.089902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453476</td>\n",
       "      <td>-0.810936</td>\n",
       "      <td>-0.243210</td>\n",
       "      <td>-0.864821</td>\n",
       "      <td>0.359710</td>\n",
       "      <td>1.711864</td>\n",
       "      <td>-0.356597</td>\n",
       "      <td>-0.168307</td>\n",
       "      <td>0.628820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>dr</td>\n",
       "      <td>-0.792492</td>\n",
       "      <td>-1.243818</td>\n",
       "      <td>0.944725</td>\n",
       "      <td>-0.359393</td>\n",
       "      <td>-0.139831</td>\n",
       "      <td>-0.424244</td>\n",
       "      <td>0.061578</td>\n",
       "      <td>0.682457</td>\n",
       "      <td>-0.623598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650828</td>\n",
       "      <td>-0.774872</td>\n",
       "      <td>0.994104</td>\n",
       "      <td>-0.298543</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>-0.035435</td>\n",
       "      <td>-0.358144</td>\n",
       "      <td>0.346977</td>\n",
       "      <td>1.121911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>seuss</td>\n",
       "      <td>0.351036</td>\n",
       "      <td>0.141784</td>\n",
       "      <td>-0.324931</td>\n",
       "      <td>-0.464101</td>\n",
       "      <td>-0.225673</td>\n",
       "      <td>-0.261663</td>\n",
       "      <td>-0.822348</td>\n",
       "      <td>0.883749</td>\n",
       "      <td>-0.537540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925906</td>\n",
       "      <td>-0.332249</td>\n",
       "      <td>0.790110</td>\n",
       "      <td>-0.503354</td>\n",
       "      <td>0.752054</td>\n",
       "      <td>0.694911</td>\n",
       "      <td>0.176286</td>\n",
       "      <td>-0.718676</td>\n",
       "      <td>1.355223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3463 rows × 98 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42cfa28b",
    "outputId": "b28716f3-f4f0-45f4-e2cd-4a11ef546f26",
    "ExecuteTime": {
     "end_time": "2025-11-26T18:47:27.364094Z",
     "start_time": "2025-11-26T18:47:27.357272Z"
    }
   },
   "source": [
    "print(embeding[['word', 'count']].sort_values(by='count', ascending=False).head(20).to_markdown())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      | word         |   count |\n",
      "|-----:|:-------------|--------:|\n",
      "|   72 | trump        |     164 |\n",
      "|  220 | house        |      31 |\n",
      "|   29 | donald_trump |      30 |\n",
      "|  219 | white        |      26 |\n",
      "|  189 | black        |      22 |\n",
      "|  258 | man          |      22 |\n",
      "| 1011 | deal         |      21 |\n",
      "|  273 | people       |      20 |\n",
      "|  136 | twitter      |      19 |\n",
      "|  132 | day          |      19 |\n",
      "|  129 | report       |      19 |\n",
      "|   37 | sexual       |      19 |\n",
      "|  419 | gay          |      18 |\n",
      "|  351 | star         |      18 |\n",
      "|  126 | women        |      18 |\n",
      "|  267 | iran         |      18 |\n",
      "|  459 | wedding      |      17 |\n",
      "|  262 | gun          |      17 |\n",
      "|  341 | john         |      17 |\n",
      "|  572 | primary      |      17 |\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "PEG9up58OzVZ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ede93ed"
   },
   "source": [
    "## Analyse des Résultats :\n",
    "\n",
    "### 1. Chargement et première exploration des données\n",
    "\n",
    "J'ai commencé par charger le jeu de données `News_Category_Dataset_v2.json` en utilisant `pandas`. C'est un fichier JSON Lines, donc j'ai dû m'assurer de le lire correctement en gérant les lignes potentiellement mal formées pour ne pas bloquer le chargement. Une fois chargé, j'ai limité le jeu de données à 1000 entrées pour les tests et j'ai affiché les premières lignes (`dataset.head()`) pour me familiariser avec sa structure : on y trouve la catégorie, le titre (`headline`), les auteurs, etc.\n",
    "\n",
    "### 2. Premier comptage de mots (avant tout prétraitement)\n",
    "\n",
    "Pour avoir une idée de ce que contient le corpus, j'ai utilisé une fonction `dummy_word_split` très simple (qui sépare les mots par les espaces) et une fonction `compute_word_occurences` pour compter la fréquence des mots. J'ai affiché les 20 mots les plus fréquents, et comme on pouvait s'y attendre, c'était plein de mots très courants comme 'To', 'The', 'Of', 'In', 'A', 'For'. Clairement, cette approche naïve ne nous donne pas beaucoup d'informations sur les sujets réels des nouvelles, c'était surtout du bruit.\n",
    "\n",
    "### 3. Construction du pipeline de prétraitement\n",
    "\n",
    "C'est là que le gros du travail a commencé ! J'ai suivi les étapes définies pour nettoyer et transformer le texte :\n",
    "\n",
    "*   **Assurer la qualité des données** : J'ai mis en place une vérification simple pour m'assurer que les données étaient dans le bon format, en forçant les titres en chaînes de caractères. Le dataset a passé ce contrôle sans problème.\n",
    "\n",
    "*   **Filtrage des textes** : J'ai créé une fonction `filter_text` pour supprimer des éléments indésirables comme les URLs, les tags HTML, les adresses e-mail, les dates et la ponctuation, ainsi que les chiffres. Cela a rendu les titres beaucoup plus propres.\n",
    "\n",
    "*   **Unification des textes et tokenisation** : J'ai utilisé une fonction `sent_to_words` (qui a été un peu ajustée en cours de route) pour mettre tous les mots en minuscules, supprimer les accents avec `unidecode`, et surtout, pour diviser correctement chaque titre en une liste de mots individuels en utilisant `gensim.utils.simple_preprocess`. C'était crucial pour la suite, car avant ça, mes listes de mots étaient vides !\n",
    "\n",
    "*   **Suppression des mots inutiles (Stopwords)** : J'ai chargé une liste de mots-vides (`stopwords.txt` et la liste `ENGLISH_STOP_WORDS` de `sklearn`) et j'ai supprimé ces mots très courants de mes listes de mots. Cela a permis de se concentrer sur les termes plus significatifs.\n",
    "\n",
    "*   **Création de N-grammes** : J'ai appliqué la création de bigrammes (mots qui apparaissent souvent ensemble) en utilisant `gensim.models.phrases`. Par exemple, des expressions comme `donald_trump` ont été identifiées et traitées comme un seul terme, ce qui est bien plus pertinent pour comprendre les sujets que les mots 'donald' et 'trump' séparément.\n",
    "\n",
    "*   **Lemmatisation et filtrage des parties du discours (PoS)** : J'ai défini une fonction `lemmatize_texts` qui utilise spaCy pour réduire les mots à leur forme de base (lemme) et filtrer selon les parties du discours (par exemple, ne garder que les noms, verbes, adjectifs, adverbes). Bien que cette fonction n'ait pas été appliquée directement pour le calcul des occurrences finales que j'ai affichées, le concept de lemmatisation est essentiel pour regrouper les variations d'un même mot.\n",
    "\n",
    "### 4. Génération et analyse des *Word Embeddings*\n",
    "\n",
    "Enfin, j'ai généré des *word embeddings* pour les mots de mon corpus prétraité en utilisant `spacy` et son modèle `en_core_web_sm`. Ces embeddings sont des vecteurs numériques qui représentent le sens des mots, où des mots sémantiquement similaires sont proches dans l'espace vectoriel.\n",
    "\n",
    "J'ai ensuite utilisé la fonction `get_word_embeddings` pour créer un DataFrame `embeding` qui contient chaque mot, ses 100 dimensions d'embedding, et son compte d'occurrences. En affichant les 20 mots les plus fréquents de ce DataFrame, j'ai pu constater une amélioration spectaculaire par rapport au premier comptage : on voit maintenant des mots comme `trump`, `house`, `donald_trump`, `white`, `man`, `black`, qui sont beaucoup plus riches en information et pertinents pour la modélisation de sujets. Le pipeline de prétraitement a clairement fait son travail en mettant en lumière les termes clés du corpus.\n",
    "\n",
    "### En résumé pour la modélisation de sujets\n",
    "\n",
    "Grâce à toutes ces étapes, j'ai transformé les titres de nouvelles bruts en une représentation propre et sémantiquement riche. Nous avons une liste de mots pertinents, et l'impact du prétraitement est flagrant sur les termes les plus fréquents. Cette base de données de mots avec leurs embeddings et leurs fréquences est maintenant une excellente fondation pour aborder la tâche finale de modélisation de sujets, car nous avons éliminé le bruit et mis en avant le contenu informatif."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "b1Ygdc2uPlxb"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "TP_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "mezA-O2Fzb2k"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
