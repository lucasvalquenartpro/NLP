{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e039b0e",
   "metadata": {
    "id": "5e039b0e"
   },
   "source": [
    "# <center>Natural Language Processing Hands-on # 1</center>\n",
    "<center><span style=\"font-weight: bold; font-size: 1.8rem;\">Representing words and sentences</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967fd03",
   "metadata": {
    "id": "6967fd03"
   },
   "source": [
    "Since most of the algorithms existing out there are designed to handle numerical data, they are hardly applicable on raw texts. However, it is definitely possible to convert a text to a numerical representation.\n",
    "\n",
    "Ideal representations should handle **semantic**, **polysemy**, **irony** and lots of other specificities of texts. Along the decades, many text representations have been introduced to handle as many specificities as possible.\n",
    "\n",
    "In this hands-on, you will have to convert a given corpus of texts to various representations and highlight their pros / cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51c251",
   "metadata": {
    "id": "7f51c251"
   },
   "source": [
    "# Installation of required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b47516",
   "metadata": {
    "id": "36b47516"
   },
   "source": [
    "The packages listed below should be installed. Using a virtual environment is highly recommended but not mandatory -- that is just good practice."
   ]
  },
  {
   "cell_type": "code",
   "id": "6HVnaVEgv9pT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HVnaVEgv9pT",
    "outputId": "1e2e900c-84b9-4f1f-e297-44c7c5a97eef",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:05.800202Z",
     "start_time": "2025-11-26T17:51:05.684376Z"
    }
   },
   "source": [
    "!pip install gensim"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "efb6e6f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efb6e6f8",
    "outputId": "ba253fd7-c8a4-4a2a-a813-24ced2bf98ff",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:09.507176Z",
     "start_time": "2025-11-26T17:51:08.046109Z"
    }
   },
   "source": [
    "import random\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "# Uncomment the following line to download the reuters dataset\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasvalquenart/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/lucasvalquenart/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "663eb3ae",
   "metadata": {
    "id": "663eb3ae"
   },
   "source": [
    "**Note**: In NLP, we often add `<START>` and `<END>` tokens to represent the beginning and end of sentences, paragraphs or documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a73a92",
   "metadata": {
    "id": "f0a73a92"
   },
   "source": [
    "# Part 0 - Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777a255",
   "metadata": {
    "id": "d777a255"
   },
   "source": [
    "The Reuters Corpus that we will use contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 categories.\n",
    "\n",
    "Before diving into word representations, let's explore it a little bit and simply preprocess its texts to make it more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac27760",
   "metadata": {
    "id": "5ac27760"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3f7a0",
   "metadata": {
    "id": "a3b3f7a0"
   },
   "source": [
    "We will need to standardize all texts before converting anything to a numerical representations, since it will reduce the vocabulary size. Modify the following function to:\n",
    "\n",
    "* Add the `START_TOKEN` and `END_TOKEN` at the beginning and end of each document\n",
    "* Lowercase every words\n",
    "* Remove the punctuation from each document"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7f1c69c",
   "metadata": {
    "id": "e7f1c69c",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:12.372238Z",
     "start_time": "2025-11-26T17:51:12.367986Z"
    }
   },
   "source": [
    "def read_corpus(category=\"tea\", add_token=True):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "\n",
    "    #convert all words to lowercase and remove ponctuation\n",
    "    corpus = [[w.lower().translate(str.maketrans(\"\", \"\", string.punctuation)) for w in list(reuters.words(f))] for f in files]\n",
    "    corpus = [[word for word in doc if word and not word.isnumeric()] for doc in corpus]\n",
    "\n",
    "    #add token if necessary\n",
    "    if add_token:\n",
    "        corpus = [[START_TOKEN] + doc + [END_TOKEN] for doc in corpus if doc]\n",
    "    return corpus"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9d9c36a4",
   "metadata": {
    "id": "9d9c36a4",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:13.515371Z",
     "start_time": "2025-11-26T17:51:13.096321Z"
    }
   },
   "source": [
    "corpus = read_corpus()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "946e85d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "946e85d1",
    "outputId": "56c7ea8f-ade8-499d-a6d1-54a98be97266",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:14.030395Z",
     "start_time": "2025-11-26T17:51:14.028651Z"
    }
   },
   "source": [
    "#pp(corpus[:2], compact=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "f009fb2f",
   "metadata": {
    "id": "f009fb2f"
   },
   "source": [
    "# Part I - Word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9311f",
   "metadata": {
    "id": "c1c9311f"
   },
   "source": [
    "Now that we have preprocessed our texts we can represent them using vectors, also called embeddings in this case.\n",
    "\n",
    "*Note: the preprocessing done here is basic. We will see in another hands-on different preprocessing steps, including some suitable for frequentist approaches.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda63c6",
   "metadata": {
    "id": "3bda63c6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575fee0b",
   "metadata": {
    "id": "575fee0b"
   },
   "source": [
    "Each word representation has its pros and cons: understanding them will help you in finding the best representation that suits your use case.\n",
    "\n",
    "As a result, you will have to implement / load and analyze the behaviour of word vectors coming from:\n",
    "\n",
    "* Dummy encoding\n",
    "* Co-occurence matrix encoding\n",
    "* Pretrained GloVe encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366603e",
   "metadata": {
    "id": "8366603e"
   },
   "source": [
    "## Dummy encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bf1b1",
   "metadata": {
    "id": "2f4bf1b1"
   },
   "source": [
    "The dummy encoding consist in encoding each individual word of our corpus into a vector filled with 0 expect at a specific position where the value is 1 (equivalent to the encoding of categorical variables).\n",
    "\n",
    "As discussed during the course, those embeddings are kind of pointless since they don't handle a single element of the ideal word representation beside being actual vectors. They are however a good starting point to play around with texts.\n",
    "\n",
    "---\n",
    "\n",
    "Define a function converting the words of a corpus to a set of dummy encoded vectors. Do not forget to sort your vocabulary before assigning the vectors!"
   ]
  },
  {
   "cell_type": "code",
   "id": "d37d87b6",
   "metadata": {
    "id": "d37d87b6",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:15.632525Z",
     "start_time": "2025-11-26T17:51:15.630253Z"
    }
   },
   "source": [
    "def dummy_encode(corpus):\n",
    "    \"\"\"One-hot encoding of a set of texts.\n",
    "    \n",
    "    Params:\n",
    "        corpus (list of lists): list of tokenized documents, where each document \n",
    "                                is a list of words\n",
    "    Return:\n",
    "        dict: dictionary mapping each unique word to its one-hot encoded vector\n",
    "    \"\"\"\n",
    "    # extract all unique words from corpus and sort them alphabetically\n",
    "    words = sorted(list(set(itertools.chain.from_iterable(corpus))))\n",
    "    embedding = np.eye(len(words))\n",
    "\n",
    "    return {words: embedding[i] for i, words in enumerate(words)}"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "d84fd911",
   "metadata": {
    "id": "d84fd911",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:15.796396Z",
     "start_time": "2025-11-26T17:51:15.794737Z"
    }
   },
   "source": [
    "#pp(dummy_encode(corpus[:2]), compact=True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "40953475",
   "metadata": {
    "id": "40953475"
   },
   "source": [
    "If you still do not believe that this representation is pointless, try finding the most similar word to \"cat\" using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212367c",
   "metadata": {
    "id": "d212367c"
   },
   "source": [
    "## Co-occurence matrix encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d7908",
   "metadata": {
    "id": "711d7908"
   },
   "source": [
    "*This section comes from Stanford's NLP hands-on*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830989b",
   "metadata": {
    "id": "2830989b"
   },
   "source": [
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word  $w_i$  occurring in the document, we consider the context window surrounding  $w_i$ . Supposing our fixed window size is  $n$ , then this is the  $n$  preceding and  $n$  subsequent words in that document, i.e. words  $w_{i−n} … w_{i−1}$  and  $w_{i+1} … w{i+n}$ . We build a co-occurrence matrix  $M$ , which is a symmetric word-by-word matrix in which  $M_{ij}$  is the number of times  $w_j$  appears inside  $w_i$ 's window among all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66869c4",
   "metadata": {
    "id": "f66869c4"
   },
   "source": [
    "**Example: Co-Occurrence with Fixed Window of n=1:**\n",
    "\n",
    "* Document 1: \"all that glitters is not gold\"\n",
    "\n",
    "* Document 2: \"all is well that ends well\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f2b4c",
   "metadata": {
    "id": "860f2b4c"
   },
   "source": [
    "|        * \t| <START> \t| all \t| that \t| glitters \t| is \t| not \t| gold \t| well \t| ends \t| <END> \t|\n",
    "|---------:\t|--------:\t|----:\t|-----:\t|---------:\t|---:\t|----:\t|-----:\t|-----:\t|-----:\t|------:\t|\n",
    "|  <START> \t|       0 \t|   2 \t|    0 \t|        0 \t|  0 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|      all \t|       2 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|     that \t|       0 \t|   1 \t|    0 \t|        1 \t|  0 \t|   0 \t|    0 \t|    1 \t|    1 \t|     0 \t|\n",
    "| glitters \t|       0 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|       is \t|       0 \t|   1 \t|    0 \t|        1 \t|  0 \t|   1 \t|    0 \t|    1 \t|    0 \t|     0 \t|\n",
    "|      not \t|       0 \t|   0 \t|    0 \t|        0 \t|  1 \t|   0 \t|    1 \t|    0 \t|    0 \t|     0 \t|\n",
    "|     gold \t|       0 \t|   0 \t|    0 \t|        0 \t|  0 \t|   1 \t|    0 \t|    0 \t|    0 \t|     1 \t|\n",
    "|     well \t|       0 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    1 \t|     1 \t|\n",
    "|     ends \t|       0 \t|   0 \t|    1 \t|        0 \t|  0 \t|   0 \t|    0 \t|    1 \t|    0 \t|     0 \t|\n",
    "|    <END> \t|       0 \t|   0 \t|    0 \t|        0 \t|  0 \t|   0 \t|    1 \t|    1 \t|    0 \t|     0 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b153b8",
   "metadata": {
    "id": "20b153b8"
   },
   "source": [
    "The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run dimensionality reduction. In particular, we will run *SVD* (Singular Value Decomposition), which is a kind of generalized *PCA* (Principal Components Analysis) to select the top  k  principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc6263",
   "metadata": {
    "id": "20bc6263"
   },
   "source": [
    "Reducing the dimensionality of such vectors doesn't interterfere with the semantic relationship between words. Hence, *movie* will still be closer to *theater* than to *airplane*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41120cd",
   "metadata": {
    "id": "f41120cd"
   },
   "source": [
    "### Identify distinct words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf508598",
   "metadata": {
    "id": "cf508598"
   },
   "source": [
    "Define a function that will return a list of unique words of the corpus as well as its size."
   ]
  },
  {
   "cell_type": "code",
   "id": "a376f246",
   "metadata": {
    "id": "a376f246",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:19.598812Z",
     "start_time": "2025-11-26T17:51:19.595918Z"
    }
   },
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    #extract unique words\n",
    "    corpus_words = sorted(list(set(itertools.chain.from_iterable(corpus))))\n",
    "    num_corpus_words = len(corpus_words)\n",
    "    return corpus_words, num_corpus_words"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "fe942f5a606e596c",
   "metadata": {
    "id": "fe942f5a606e596c",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:20.187199Z",
     "start_time": "2025-11-26T17:51:20.184985Z"
    }
   },
   "source": [
    "#print(distinct_words(corpus[:2]))"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "49ce9e6d",
   "metadata": {
    "id": "49ce9e6d"
   },
   "source": [
    "### Compute the co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3bda31",
   "metadata": {
    "id": "8a3bda31"
   },
   "source": [
    "Write a method that constructs a co-occurrence matrix for a certain window-size  $n$  (with a default of $4$), considering words  $n$  before and  $n$  after the word in the center of the window."
   ]
  },
  {
   "cell_type": "code",
   "id": "1760c654",
   "metadata": {
    "id": "1760c654",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:21.249273Z",
     "start_time": "2025-11-26T17:51:21.245031Z"
    }
   },
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "\n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "\n",
    "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
    "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)):\n",
    "                Co-occurence matrix of word counts.\n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "\n",
    "    M = np.zeros((num_words, num_words))\n",
    "    #word to index matrix\n",
    "    for idx, word in enumerate(words):\n",
    "        word2Ind[word] = idx\n",
    "    #co-occurence matrix (use to capture word relationships (ex : king-queen))\n",
    "    for doc in corpus:\n",
    "        for i, word in enumerate(doc):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            for j in range(start, i):\n",
    "              if i != j:\n",
    "                M[word2Ind[doc[j]], word2Ind[word]] += 1\n",
    "\n",
    "    return M, word2Ind"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "27q_yoI1zbDe",
   "metadata": {
    "id": "27q_yoI1zbDe",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:21.721529Z",
     "start_time": "2025-11-26T17:51:21.713629Z"
    }
   },
   "source": [
    "M, word2Ind = compute_co_occurrence_matrix(corpus, window_size=4)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "4a14ce49",
   "metadata": {
    "id": "4a14ce49"
   },
   "source": [
    "### Reduce the dimensionality of the co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b28d3f",
   "metadata": {
    "id": "f7b28d3f"
   },
   "source": [
    "Construct a method that performs dimensionality reduction on the matrix to produce $k$-dimensional embeddings. Use *SVD* to take the top $k$ components and produce a new matrix of $k$-dimensional embeddings.\n",
    "\n",
    "In our case, we will set $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "id": "f89c3f27",
   "metadata": {
    "id": "f89c3f27",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:22.830354Z",
     "start_time": "2025-11-26T17:51:22.827423Z"
    }
   },
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"\n",
    "    n_iters = 10\n",
    "    M_reduced = None\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=n_iters, random_state=42)\n",
    "    svd.fit(M)\n",
    "    M_reduced = svd.transform(M)\n",
    "    return M_reduced"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "zl9aMLaPzYG-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zl9aMLaPzYG-",
    "outputId": "575e2826-e70b-4620-ed04-d7b64e6e472c",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:23.480677Z",
     "start_time": "2025-11-26T17:51:23.173857Z"
    }
   },
   "source": [
    "M_reduced_co_occurrence = reduce_to_k_dim(M)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "6b1938d1",
   "metadata": {
    "id": "6b1938d1"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2097a",
   "metadata": {
    "id": "2ba2097a"
   },
   "source": [
    "Great! You now have fix-sized vectors that represent each words of your corpus. Let's normalize our matrix to compare our vectors easily."
   ]
  },
  {
   "cell_type": "code",
   "id": "00217789",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00217789",
    "outputId": "5f012e68-3697-4e7b-b9f9-e3f62c85fd44",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:24.637607Z",
     "start_time": "2025-11-26T17:51:24.635537Z"
    }
   },
   "source": [
    "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h8/4yjlz80128b_m2pjqt90s0r80000gn/T/ipykernel_23152/238959648.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "6458ea3d",
   "metadata": {
    "id": "6458ea3d"
   },
   "source": [
    "Since we are working with vectors, we can easily measure the similarity between them using the dot product. Hence, given a specific word and its related word embedding, we can easily identify its most similar words contained in the corpus!\n",
    "\n",
    "*Note: the dot product between two vectors is bounded between -1 and 1 and the dot product of two identical vectors is equal to 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebb48c",
   "metadata": {
    "id": "5cebb48c"
   },
   "source": [
    "Define a function that given a word $w$ identify its most similar words in your embedding space."
   ]
  },
  {
   "cell_type": "code",
   "id": "O80X8A6S1kqa",
   "metadata": {
    "id": "O80X8A6S1kqa",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:25.732554Z",
     "start_time": "2025-11-26T17:51:25.729626Z"
    }
   },
   "source": [
    "def most_similar(word, matrix, topn=10):\n",
    "    \"\"\"Return the words that have the closest embedding to the queried word.\"\"\"\n",
    "    ranked_words = []\n",
    "    \n",
    "    # cosine similarity\n",
    "    similarity = np.dot(matrix, word)\n",
    "    #sort idx by similarity (DESC)\n",
    "    ranked_words = np.argsort(similarity)[::-1]\n",
    "    return ranked_words[:topn]"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "94aff5f5",
   "metadata": {
    "id": "94aff5f5"
   },
   "source": [
    "## GloVe encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7077ed",
   "metadata": {
    "id": "fd7077ed"
   },
   "source": [
    "Word2Vec models are predictive by essence, since it is a neural network. However, this is not the sole method to learn geometrical encodings (vectors) of words from their co-occurrence information (how frequently they appear together in large text corpora).\n",
    "\n",
    "GloVe is a count-based model that learn their vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix. Does it remind you of something? Yes, that's exactly what you did above.\n",
    "\n",
    "Building models is time consuming. Hence, GloVe / Word2Vec models already trained on regular training sets (Wikipedia, News, etc.) are publicly shared to be reused easily.\n",
    "\n",
    "The below code will load a Glove model trained on wikipedia and allow us inspect easily the embeddings properties."
   ]
  },
  {
   "cell_type": "code",
   "id": "e4897c46",
   "metadata": {
    "id": "e4897c46",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:26.805898Z",
     "start_time": "2025-11-26T17:51:26.802850Z"
    }
   },
   "source": [
    "import gensim.downloader as api"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "a872a3d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a872a3d9",
    "outputId": "bf2d5f72-73a2-4eca-c09a-a4b16e86c616",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:27.368185Z",
     "start_time": "2025-11-26T17:51:27.205862Z"
    }
   },
   "source": [
    "pp(list(api.info()[\"models\"].keys()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300',\n",
      " 'conceptnet-numberbatch-17-06-300',\n",
      " 'word2vec-ruscorpora-300',\n",
      " 'word2vec-google-news-300',\n",
      " 'glove-wiki-gigaword-50',\n",
      " 'glove-wiki-gigaword-100',\n",
      " 'glove-wiki-gigaword-200',\n",
      " 'glove-wiki-gigaword-300',\n",
      " 'glove-twitter-25',\n",
      " 'glove-twitter-50',\n",
      " 'glove-twitter-100',\n",
      " 'glove-twitter-200',\n",
      " '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "72fb7437",
   "metadata": {
    "id": "72fb7437",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:40.847119Z",
     "start_time": "2025-11-26T17:51:27.592718Z"
    }
   },
   "source": [
    "model = api.load('glove-wiki-gigaword-50')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 35.5% 23.4/66.0MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "1ae648da",
   "metadata": {
    "id": "1ae648da"
   },
   "source": [
    "You can get the most similar embeddings to those of a given set of words.\n",
    "Here, we retrieve the most similar words to fox, rabbit and cat."
   ]
  },
  {
   "cell_type": "code",
   "id": "254dd7b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "254dd7b1",
    "outputId": "0dcfe9cf-6f90-48bc-d2ad-9de0121f054d",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:40.891020Z",
     "start_time": "2025-11-26T17:51:40.851824Z"
    }
   },
   "source": [
    "pp(model.most_similar(positive=[\"fox\", \"rabbit\", \"cat\"]), compact=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 0.8569530248641968), ('mouse', 0.7859790325164795),\n",
      " ('monster', 0.7710845470428467), ('wolf', 0.7690606117248535),\n",
      " ('bunny', 0.7655251026153564), ('spider', 0.7395666241645813),\n",
      " ('duck', 0.7366621494293213), ('rat', 0.7366542220115662),\n",
      " ('beast', 0.7319128513336182), ('cartoon', 0.724099338054657)]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "aa4b5d39",
   "metadata": {
    "id": "aa4b5d39"
   },
   "source": [
    "You can also perform concept additions / soustractions. For instance, you can"
   ]
  },
  {
   "cell_type": "code",
   "id": "af07844f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af07844f",
    "outputId": "61879270-5157-40bb-9086-8cc7263aa031",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:40.952306Z",
     "start_time": "2025-11-26T17:51:40.916769Z"
    }
   },
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523604273796082),\n",
       " ('throne', 0.7664334177970886),\n",
       " ('prince', 0.759214460849762),\n",
       " ('daughter', 0.7473882436752319),\n",
       " ('elizabeth', 0.7460219860076904),\n",
       " ('princess', 0.7424570322036743),\n",
       " ('kingdom', 0.7337412238121033),\n",
       " ('monarch', 0.7214491367340088),\n",
       " ('eldest', 0.7184861898422241),\n",
       " ('widow', 0.7099431157112122)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "4ba43229",
   "metadata": {
    "id": "4ba43229"
   },
   "source": [
    "### Explore those embeddings and comment on how they help to identify / handle:\n",
    "\n",
    "* Synonyms\n",
    "* Antonyms\n",
    "* Grammatical errors\n",
    "* Polysemy\n",
    "* Irony\n",
    "* Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802965e",
   "metadata": {
    "id": "9802965e"
   },
   "source": [
    "* **Synonyms**: Word embeddings naturally group synonyms together because they appear in similar contexts. For example, \"big\" and \"large\" will have similar vectors since they're used interchangeably in text. The `most_similar()` function easily retrieves synonyms.\n",
    "\n",
    "* **Antonyms**: Antonyms often have similar embeddings too! Words like \"hot\" and \"cold\" appear in similar contexts (\"the weather is hot/cold\"), so their vectors are close. Embeddings alone cannot distinguish synonyms from antonyms without additional context.\n",
    "\n",
    "* **Grammatical errors**: Embeddings don't directly detect grammatical errors. They focus on semantic meaning, not grammar. However, they can help suggest corrections if a misspelled word is close to a correct word in the embedding space.\n",
    "\n",
    "* **Polysemy**: Embeddings struggle with polysemy . For example, \"mouse\" (computer vs. animal) gets a single vector that's an average of all its uses.\n",
    "\n",
    "* **Irony**: Embeddings cannot detect irony. Irony depends on context and tone, which static word vectors don't capture.\n",
    "\n",
    "* **Analogies**: The vector arithmetic (king - man + woman ≈ queen) demonstrates that semantic relationships are encoded geometrically. The model captures analogical reasoning through vector operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc618ec",
   "metadata": {
    "id": "5cc618ec"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc941ba6",
   "metadata": {
    "id": "bc941ba6"
   },
   "source": [
    "### Where do you think those biases come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec7793",
   "metadata": {},
   "source": [
    "Biases in word embeddings come directly from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9980b8",
   "metadata": {
    "id": "9c9980b8"
   },
   "source": [
    "# Part II - Sentence representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec10d2",
   "metadata": {
    "id": "f2ec10d2"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0688c",
   "metadata": {
    "id": "3ae0688c"
   },
   "source": [
    "The TF-IDF is a methodology aiming at finding the most significative words in each document by comparing their in-document frequency to the overall frequency of that term in the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b501c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:59:07.229677Z",
     "start_time": "2021-10-18T22:59:07.216681Z"
    },
    "id": "a3b501c5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03b771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:00:10.087467Z",
     "start_time": "2021-10-18T23:00:10.074094Z"
    },
    "id": "6d03b771"
   },
   "source": [
    "Convert the reuters corpus (at least one category) to its TF-IDF representation using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Beware of all the parameters of the methods! They might have significant impact on what you're doing."
   ]
  },
  {
   "cell_type": "code",
   "id": "5a9bd49f",
   "metadata": {
    "id": "5a9bd49f",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:40.973559Z",
     "start_time": "2025-11-26T17:51:40.970827Z"
    }
   },
   "source": [
    "corpus_string = [' '.join(doc) for doc in corpus]"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "0ca0b2a9",
   "metadata": {
    "id": "0ca0b2a9"
   },
   "source": [
    "Once you have converted your corpus using the TF-IDF methodology, create a function identifying the most relevant comments given a search query."
   ]
  },
  {
   "cell_type": "code",
   "id": "647bfc73",
   "metadata": {
    "id": "647bfc73",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:51:40.989707Z",
     "start_time": "2025-11-26T17:51:40.987181Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def search_corpus(corpus, search_query, topn=10):\n",
    "    \"\"\"Retrieve the top n documents matching a search query within a list of texts.\n",
    "\n",
    "    \"\"\"\n",
    "    tfidf_corpus = TfidfVectorizer()\n",
    "    tfidf = tfidf_corpus.fit_transform(corpus)\n",
    "\n",
    "    query_vector = tfidf_corpus.transform([search_query])\n",
    "\n",
    "    similarities = np.dot(query_vector, tfidf.T).toarray().flatten()\n",
    "    ranked_articles = np.argsort(similarities)[::-1]\n",
    "\n",
    "    return ranked_articles[:topn]"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "5tG4RNm-9Yb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tG4RNm-9Yb4",
    "outputId": "54ac84e7-a43a-4f80-890d-6623ffe5ba46",
    "ExecuteTime": {
     "end_time": "2025-11-26T17:54:34.127682Z",
     "start_time": "2025-11-26T17:54:34.119470Z"
    }
   },
   "source": [
    "top_indices = search_corpus(corpus_string, \"europe\", topn=5)\n",
    "\n",
    "for i in top_indices:\n",
    "    print(corpus[i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'soviet', 'paper', 'details', 'georgian', 'flood', 'damage', 'floods', 'and', 'avalanches', 'killed', 'people', 'and', 'caused', 'around', 'mln', 'roubles', 'worth', 'of', 'damage', 'in', 'the', 'southern', 'soviet', 'republic', 'of', 'georgia', 'earlier', 'this', 'year', 'the', 'government', 'daily', 'izvestia', 'said', 'some', 'hectares', 'of', 'agricultural', 'land', 'and', 'gardens', 'had', 'been', 'inundated', 'damaging', 'tea', 'plantations', 'and', 'orange', 'groves', 'the', 'newspaper', 'said', 'it', 'added', 'that', 'spring', 'sowing', 'in', 'southern', 'parts', 'of', 'the', 'country', 'was', 'some', 'two', 'weeks', 'behind', 'schedule', 'because', 'of', 'the', 'late', 'thaw', 'but', 'gave', 'no', 'precise', 'crop', 'estimates', 'in', 'the', 'most', 'detailed', 'report', 'to', 'date', 'on', 'the', 'heavy', 'snows', 'in', 'january', 'and', 'floods', 'in', 'february', 'izvestia', 'said', 'people', 'had', 'been', 'evacuated', 'from', 'mountain', 'areas', 'houses', 'had', 'been', 'damaged', 'and', 'hundreds', 'of', 'kilometres', 'of', 'roads', 'and', 'power', 'lines', 'had', 'been', 'destroyed', 'a', 'separate', 'article', 'in', 'the', 'daily', 'warned', 'that', 'a', 'sudden', 'thaw', 'was', 'expected', 'shortly', 'in', 'the', 'ukraine', 'and', 'southern', 'parts', 'of', 'russia', 'which', 'experienced', 'record', 'snows', 'this', 'winter', 'preventive', 'measures', 'have', 'already', 'been', 'taken', 'in', 'some', 'areas', 'including', 'the', 'evacuation', 'of', 'cattle', '<END>']\n",
      "['<START>', 'vietnam', 'to', 'resettle', 'on', 'state', 'farms', 'in', 'vietnam', 'will', 'resettle', 'people', 'on', 'state', 'farms', 'known', 'as', 'new', 'economic', 'zones', 'in', 'to', 'create', 'jobs', 'and', 'grow', 'more', 'high', 'value', 'export', 'crops', 'the', 'communist', 'party', 'newspaper', 'nhan', 'dan', 'said', 'yesterday', 's', 'edition', 'received', 'here', 'today', 'said', 'vietnam', 'would', 'invest', 'one', 'billion', 'dong', 'including', 'the', 'costs', 'of', 'relocation', 'in', 'new', 'economic', 'zones', 'about', 'one', 'third', 'of', 'that', 'sum', 'would', 'be', 'spent', 'on', 'export', 'crops', 'such', 'as', 'coffee', 'tea', 'rubber', 'and', 'pepper', 'in', 'the', 'central', 'highlands', 'it', 'said', 'since', 'vietnam', 'has', 'resettled', 'about', 'three', 'million', 'people', 'from', 'cities', 'and', 'crowded', 'river', 'deltas', 'to', 'the', 'zones', '<END>']\n",
      "['<START>', 'economic', 'spotlight', 'asian', 'droughts', 'three', 'geographically', 'diverse', 'droughts', 'in', 'asia', 'are', 'being', 'linked', 'by', 'some', 'scientists', 'to', 'a', 'reintensification', 'of', 'the', 'complex', 'and', 'little', 'understood', 'el', 'nino', 'weather', 'pattern', 'lt', 'accu', 'weather', 'inc', 'a', 'commercial', 'weather', 'forecasting', 'service', 'said', 'rice', 'and', 'wheat', 'farmers', 'in', 'china', 'wheat', 'and', 'sugarcane', 'growers', 'in', 'australia', 'and', 'tea', 'planters', 'in', 'sri', 'lanka', 'all', 'face', 'serious', 'losses', 'to', 'their', 'respective', 'harvests', 'unless', 'rains', 'arrive', 'in', 'time', 'to', 'break', 'the', 'droughts', 'offical', 'reports', 'government', 'officials', 'and', 'meteorologists', 'said', 'wen', 'wei', 'po', 'a', 'hong', 'kong', 'daily', 'with', 'close', 'peking', 'links', 'said', 'the', 'drought', 'is', 'the', 'worst', 'in', 'over', 'years', 'and', 'some', 'provinces', 'have', 'been', 'without', 'adequate', 'rainfall', 'for', 'more', 'than', 'seven', 'months', 'rice', 'planting', 'is', 'threatened', 'in', 'eight', 'provinces', 'it', 'added', 'rainfall', 'in', 'the', 'key', 'farming', 'provinces', 'of', 'henan', 'and', 'sichuan', 'was', 'pct', 'below', 'average', 'during', 'february', 'the', 'lowest', 'figure', 'for', 'over', 'years', 'the', 'paper', 'said', 'the', 'dry', 'weather', 'has', 'cut', 'stored', 'water', 'volumes', 'by', 'over', 'pct', 'compared', 'with', 'last', 'march', 'and', 'lowered', 'the', 'water', 'levels', 'of', 'many', 'rivers', 'it', 'added', 'this', 'has', 'resulted', 'in', 'reduced', 'hydro', 'electric', 'power', 'causing', 'shortages', 'to', 'industry', 'and', 'households', 'the', 'upper', 'reaches', 'of', 'the', 'yangtze', 'are', 'at', 'their', 'lowest', 'levels', 'in', 'a', 'century', 'causing', 'many', 'ships', 'to', 'run', 'aground', 'wen', 'wei', 'po', 'said', 'unusually', 'high', 'temperatures', 'have', 'also', 'been', 'reported', 'across', 'china', 'media', 'reports', 'said', 'the', 'people', 's', 'daily', 'said', 'sichuan', 'has', 'recorded', 'temperatures', 'three', 'degrees', 'celsius', 'higher', 'than', 'average', 'since', 'early', 'february', 'the', 'new', 'china', 'news', 'agency', 'said', 'the', 'average', 'december', 'temperature', 'in', 'harbin', 'in', 'the', 'northeast', 'was', 'six', 'degrees', 'higher', 'than', 'last', 'december', 'and', 'degrees', 'higher', 'than', 'december', 'severe', 'drought', 'is', 'affecting', 'about', 'one', 'third', 'of', 'sri', 'lanka', 'and', 'threatens', 'to', 'reduce', 'the', 'country', 's', 'tea', 'crop', 'ministry', 'of', 'plantation', 'industries', 'officials', 'told', 'reuters', 'in', 'australia', 'concern', 'is', 'growing', 'about', 'below', 'average', 'rainfall', 'levels', 'in', 'parts', 'of', 'the', 'sugarcane', 'belt', 'along', 'the', 'queensland', 'coast', 'and', 'in', 'western', 'australia', 's', 'wheat', 'belt', 'local', 'meteorological', 'bureau', 'officials', 'said', 'for', 'many', 'farmers', 'and', 'government', 'officials', 'the', 'fear', 'is', 'that', 'while', 'the', 'present', 'low', 'rainfall', 'does', 'not', 'yet', 'pose', 'a', 'major', 'threat', 'the', 'prospect', 'of', 'a', 'dry', 'autumn', 'winter', 'season', 'when', 'the', 'wheat', 'crop', 'is', 'in', 'its', 'early', 'stages', 'certainly', 'does', 'they', 'added', 'concern', 'is', 'heightened', 'by', 'the', 'memory', 'of', 'the', 'drought', 'which', 'devastated', 'the', 'wheat', 'crop', 'and', 'coincided', 'with', 'the', 'occurrence', 'of', 'the', 'barely', 'understood', 'weather', 'phenomenon', 'known', 'as', 'el', 'nino', 'they', 'said', 'although', 'meteorologists', 'are', 'cautious', 'about', 'linking', 'the', 'asia', 'pacific', 'region', 's', 'disrupted', 'weather', 'patterns', 'to', 'any', 'single', 'cause', 'el', 'nino', 's', 'role', 'is', 'being', 'closely', 'studied', 'they', 'said', 'accu', 'weather', 'inc', 'which', 'specialises', 'in', 'providing', 'data', 'for', 'agriculture', 'and', 'shipping', 'interests', 'said', 'each', 'el', 'nino', 'event', 'was', 'unique', 'the', 'el', 'nino', 'does', 'not', 'always', 'produce', 'the', 'same', 'effects', 'and', 'the', 'present', 'occurrence', 'is', 'much', 'less', 'pronounced', 'than', 'the', 'last', 'major', 'event', 'in', 'it', 'said', 'el', 'nino', 'spanish', 'for', 'christ', 'child', 'because', 'it', 'appears', 'around', 'christmas', 'is', 'formed', 'by', 'the', 'action', 'of', 'warm', 'air', 'bearing', 'clouds', 'and', 'rain', 'shifting', 'from', 'the', 'indonesian', 'archipelago', 'to', 'the', 'coast', 'of', 'peru', 'where', 'it', 'mingles', 'with', 'the', 'cold', 'waters', 'associated', 'with', 'the', 'peru', 'current', 'and', 'returns', 'across', 'the', 'pacific', 'as', 'the', 'trade', 'winds', 'meteorologists', 'said', 'the', 'winds', 'strengthened', 'by', 'el', 'nino', 's', 'pump', 'effect', 'raise', 'the', 'sea', 'level', 'off', 'australia', 'and', 'indonesia', 'they', 'said', 'when', 'the', 'winds', 'drop', 'the', 'ocean', 'seeking', 'equilibrium', 'sends', 'a', 'surge', 'of', 'warmer', 'water', 'back', 'across', 'the', 'pacific', 'where', 'it', 'collides', 'with', 'the', 'cold', 'seas', 'off', 'peru', 'they', 'said', 'one', 'effect', 'of', 'this', 'heat', 'exchange', 'is', 'to', 'deflect', 'the', 'rain', 'bearing', 'clouds', 'away', 'from', 'australia', 'and', 'indonesia', 'into', 'the', 'pacific', 'where', 'they', 'further', 'disrupt', 'other', 'weather', 'patterns', 'the', 'prospects', 'for', 'an', 'end', 'to', 'the', 'droughts', 'vary', 'accu', 'weather', 'said', 'china', 'where', 'the', 'affected', 'areas', 'have', 'received', 'between', 'and', 'pct', 'of', 'normal', 'rainfall', 'will', 'have', 'to', 'wait', 'for', 'the', 'may', 'september', 'rains', 'it', 'said', 'the', 'may', 'september', 'rains', 'normally', 'provide', 'the', 'drought', 'striken', 'areas', 'with', 'pct', 'of', 'annual', 'rainfall', 'in', 'australia', 'areas', 'of', 'queensland', 's', 'coastal', 'strip', 'have', 'received', 'less', 'than', 'half', 'the', 'normal', 'rainfall', 'during', 'the', 'current', 'wet', 'season', 'but', 'prospects', 'for', 'increased', 'rains', 'are', 'diminishing', 'as', 'the', 'rainy', 'season', 'draws', 'to', 'an', 'end', 'in', 'sri', 'lanka', 'the', 'drought', 'has', 'come', 'when', 'rainfall', 'should', 'be', 'at', 'its', 'maximum', 'for', 'the', 'year', 'the', 'year', 's', 'secondary', 'rains', 'usually', 'occur', 'between', 'april', 'and', 'june', 'although', 'it', 'is', 'not', 'possible', 'at', 'this', 'stage', 'to', 'forecast', 'whether', 'they', 'will', 'arrive', 'as', 'usual', '<END>']\n",
      "['<START>', 'indonesian', 'tea', 'cocoa', 'exports', 'seen', 'up', 'coffee', 'down', 'indonesia', 's', 'exports', 'of', 'tea', 'and', 'cocoa', 'will', 'continue', 'to', 'rise', 'in', 'calendar', 'but', 'coffee', 'exports', 'are', 'forecast', 'to', 'dip', 'slightly', 'in', 'april', 'march', 'as', 'the', 'government', 'tries', 'to', 'improve', 'quality', 'the', 'u', 's', 'embassy', 'said', 'the', 'embassy', 's', 'annual', 'report', 'on', 'indonesian', 'agriculture', 'forecast', 'coffee', 'output', 'in', 'would', 'be', 'mln', 'bags', 'of', 'kilograms', 'each', 'that', 'is', 'slightly', 'less', 'than', 'the', 'mln', 'bags', 'produced', 'in', 'in', 'coffee', 'production', 'is', 'forecast', 'to', 'rise', 'again', 'to', 'mln', 'bags', 'but', 'exports', 'to', 'dip', 'to', 'mln', 'from', 'around', 'mln', 'in', 'exports', 'in', 'were', 'mln', 'bags', 'the', 'embassy', 'report', 'says', 'coffee', 'stocks', 'will', 'rise', 'to', 'mln', 'tonnes', 'in', 'from', 'mln', 'in', 'it', 'bases', 'this', 'on', 'a', 'fall', 'in', 'exports', 'as', 'a', 'result', 'of', 'the', 'probable', 're', 'introduction', 'of', 'quotas', 'by', 'the', 'international', 'coffee', 'organisation', 'cocoa', 'production', 'and', 'exports', 'are', 'forecast', 'to', 'rise', 'steadily', 'as', 'the', 'government', 'develops', 'cocoa', 'plantations', 'production', 'of', 'cocoa', 'in', 'indonesia', 'increased', 'to', 'tonnes', 'in', 'calendar', 'from', 'tonnes', 'in', 'it', 'is', 'projected', 'by', 'the', 'government', 'to', 'rise', 'to', 'more', 'than', 'tonnes', 'by', 'production', 'in', 'is', 'estimated', 'by', 'the', 'embassy', 'at', 'tonnes', 'as', 'against', 'tonnes', 'in', 'the', 'report', 'forecasts', 'cocoa', 'exports', 'to', 'rise', 'to', 'tonnes', 'this', 'year', 'from', 'tonnes', 'in', 'and', 'in', 'the', 'netherlands', 'is', 'at', 'present', 'the', 'biggest', 'importer', 'of', 'indonesian', 'cocoa', 'beans', 'the', 'report', 'forecasts', 'that', 'in', 'calendar', 'indonesia', 's', 'ctc', 'crushed', 'torn', 'and', 'curled', 'tea', 'exports', 'will', 'increase', 'significantly', 'with', 'the', 'coming', 'on', 'stream', 'of', 'at', 'least', 'eight', 'new', 'ctc', 'processing', 'plants', 'indonesia', 'plans', 'to', 'diversify', 'its', 'tea', 'products', 'by', 'producing', 'more', 'ctc', 'tea', 'the', 'main', 'component', 'of', 'tea', 'bags', 'production', 'of', 'black', 'and', 'green', 'teas', 'is', 'forecast', 'in', 'the', 'embassy', 'report', 'to', 'rise', 'to', 'tonnes', 'in', 'calendar', 'from', 'tonnes', 'in', 'exports', 'of', 'these', 'teas', 'are', 'likely', 'to', 'rise', 'to', 'tonnes', 'in', 'from', 'in', 'and', 'around', 'in', 'the', 'embassy', 'noted', 'the', 'ministry', 'of', 'trade', 'tightened', 'quality', 'controls', 'on', 'tea', 'in', 'october', 'in', 'an', 'effort', 'to', 'become', 'more', 'competititve', 'in', 'the', 'world', 'market', '<END>']\n",
      "['<START>', 'indonesian', 'agriculture', 'growth', 'expected', 'to', 'slow', 'indonesia', 's', 'agriculture', 'sector', 'will', 'grow', 'by', 'just', 'pct', 'in', 'calendar', 'against', 'an', 'estimated', 'pct', 'in', 'as', 'the', 'production', 'of', 'some', 'commodities', 'stagnates', 'or', 'declines', 'the', 'u', 's', 'embassy', 'said', 'in', 'a', 'report', 'production', 'of', 'indonesia', 's', 'staple', 'food', 'rice', 'is', 'forecast', 'to', 'fall', 'to', 'around', 'mln', 'tonnes', 'from', 'an', 'embassy', 'estimate', 'of', 'mln', 'tonnes', 'in', 'according', 'to', 'the', 'annual', 'report', 'on', 'indonesia', 's', 'agricultural', 'performance', 'the', 'government', 'officially', 'estimates', 'rice', 'production', 'at', 'mln', 'tonnes', 'with', 'a', 'forecast', 'mln', 'tonnes', 'output', 'in', 'the', 'report', 'says', 'wheat', 'imports', 'are', 'likely', 'to', 'fall', 'to', 'mln', 'tonnes', 'in', 'calendar', 'from', 'mln', 'tonnes', 'in', 'because', 'of', 'a', 'drawdown', 'on', 'stocks', 'growth', 'prospects', 'for', 'agriculture', 'in', 'do', 'not', 'look', 'promising', 'as', 'rice', 'production', 'is', 'forecast', 'to', 'decline', 'and', 'the', 'production', 'of', 'sugarcane', 'rubber', 'and', 'copra', 'show', 'little', 'or', 'no', 'gain', 'the', 'report', 'says', 'the', 'modest', 'overall', 'increase', 'which', 'is', 'expected', 'will', 'be', 'due', 'to', 'significant', 'gains', 'in', 'production', 'of', 'corn', 'soybeans', 'palm', 'oil', 'and', 'palm', 'kernels', 'constraints', 'to', 'significant', 'overall', 'increases', 'in', 'agricultural', 'output', 'include', 'a', 'shortage', 'of', 'disease', 'resistant', 'seeds', 'limited', 'fertile', 'land', 'insect', 'pests', 'and', 'a', 'reluctance', 'by', 'farmers', 'to', 'shift', 'from', 'rice', 'production', 'to', 'other', 'crops', 'the', 'report', 'underlines', 'the', 'fall', 'in', 'rice', 'production', 'is', 'caused', 'by', 'an', 'outbreak', 'of', 'pests', 'known', 'as', 'wereng', 'or', 'brown', 'plant', 'hoppers', 'in', 'which', 'largely', 'offset', 'gains', 'in', 'yields', 'the', 'outbreak', 'has', 'forced', 'the', 'government', 'to', 'ban', 'the', 'use', 'of', 'insecticides', 'on', 'rice', 'because', 'it', 'was', 'believed', 'the', 'wereng', 'are', 'now', 'resistant', 'to', 'these', 'varieties', 'and', 'to', 'use', 'lower', 'yielding', 'more', 'resistant', 'rice', 'types', 'the', 'government', 'is', 'depending', 'on', 'increased', 'production', 'of', 'export', 'commodities', 'such', 'as', 'coffee', 'tea', 'rubber', 'plywood', 'and', 'palm', 'oil', 'to', 'offset', 'revenue', 'losses', 'brought', 'on', 'by', 'falling', 'crude', 'oil', 'prices', 'palm', 'oil', 'production', 'is', 'expected', 'to', 'increase', 'by', 'over', 'pct', 'in', 'to', 'mln', 'tonnes', 'from', 'mln', 'with', 'exports', 'rising', 'to', 'an', 'estimated', 'tonnes', 'from', 'tonnes', 'in', 'the', 'report', 'says', 'but', 'while', 'production', 'of', 'soybeans', 'in', 'oct', 'sept', 'will', 'rise', 'to', 'mln', 'tonnes', 'from', 'in', 'imports', 'will', 'also', 'rise', 'to', 'supply', 'a', 'new', 'soybean', 'crushing', 'plant', 'the', 'report', 'says', 'that', 'imports', 'of', 'wheat', 'soybeans', 'soybean', 'meal', 'and', 'cotton', 'are', 'not', 'likely', 'to', 'decline', 'as', 'a', 'result', 'of', 'last', 'september', 's', 'pct', 'devaluation', 'of', 'the', 'rupiah', 'because', 'of', 'a', 'rise', 'in', 'domestic', 'demand', 'the', 'report', 'said', 'that', 'indonesia', 's', 'overall', 'economic', 'performance', 'in', 'calendar', 'was', 'about', 'zero', 'or', 'even', 'a', 'slight', 'negative', 'growth', 'rate', 'the', 'lowest', 'rate', 'of', 'growth', 'since', 'the', 'mid', '1960s', 'it', 'compares', 'with', 'pct', 'growth', 'in', 'and', 'pct', 'in', 'the', 'dramatic', 'fall', 'in', 'oil', 'prices', 'last', 'year', 'was', 'responsible', 'for', 'the', 'slump', '<END>']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "cc23a197",
   "metadata": {
    "id": "cc23a197"
   },
   "source": [
    "## Unsupervised Random Walk Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb91d27",
   "metadata": {
    "id": "ffb91d27"
   },
   "source": [
    "This approach has been presented by Kawin Ethayarajh in 2018. The key idea behind this methodology is to take a weighted average of previously trained word embeddings and modify it with SVD (Singular Value Decomposition, a kind of generalization of the PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c36d8",
   "metadata": {
    "id": "e78c36d8"
   },
   "source": [
    "By having a look at [this implementation](https://github.com/kawine/usif), try to compute the uSIF embeddings of our corpus and compare their properties to the TF-IDF / Averaged Word2Vec ones.\n",
    "\n",
    "Consider digging in the [related paper](https://aclanthology.org/W18-3012.pdf) if you want to know more about the methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a6d2f",
   "metadata": {
    "id": "027a6d2f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a307be0f",
   "metadata": {
    "id": "a307be0f"
   },
   "source": [
    "# Analyse et Compréhension des résultats obtenus\n",
    "\n",
    "### Introduction\n",
    "Nous avons commencé par comprendre l'importance de représenter les textes numériquement pour les algorithmes. Les textes bruts ne sont pas exploitables directement, d'où la nécessité de les convertir en vecteurs. L'objectif était d'explorer différentes méthodes de représentation de mots et de phrases, en soulignant leurs avantages et inconvénients.\n",
    "\n",
    "### Installation et Préparation\n",
    "Nous avons installé les packages nécessaires comme `gensim` et `nltk`, puis importé diverses bibliothèques (`numpy`, `pandas`, `scipy`, `sklearn`, etc.). Nous avons également téléchargé le corpus Reuters pour travailler avec. Une étape cruciale a été la définition des tokens `<START>` et `<END>` pour marquer le début et la fin des documents, ce qui est une bonne pratique en NLP.\n",
    "\n",
    "### Partie 0 - Exploration du jeu de données\n",
    "Cette partie a consisté à se familiariser avec le corpus Reuters, composé de 10 788 documents d'actualité. Nous avons implémenté la fonction `read_corpus` pour prétraiter les textes. Cette fonction est essentielle car elle permet de :\n",
    "*   Ajouter les tokens `<START>` et `<END>`.\n",
    "*   Convertir tous les mots en minuscules pour standardiser le vocabulaire.\n",
    "*   Supprimer la ponctuation, ce qui réduit le bruit et la taille du vocabulaire.\n",
    "*   Filtrer les mots vides ou numériques non pertinents.\n",
    "\n",
    "Le `pp(corpus[:2])` nous a permis de visualiser le résultat de ce prétraitement, montrant des documents bien structurés avec les tokens ajoutés, les mots en minuscules et sans ponctuation.\n",
    "\n",
    "### Partie I - Représentations de mots\n",
    "C'est le cœur du notebook, où nous avons exploré différentes manières de convertir des mots en vecteurs.\n",
    "\n",
    "#### Encadrage Dummy (One-Hot Encoding)\n",
    "Nous avons créé la fonction `dummy_encode` qui assigne un vecteur unique de zéros et un à chaque mot. J'ai compris que bien que simple, cette méthode est \"inutile\" pour capturer le sens car elle ne représente aucune relation sémantique entre les mots. Chaque mot est orthogonal à tous les autres, ce qui ne permet pas de trouver de similarités.\n",
    "\n",
    "#### Encadrage par matrice de co-occurrence\n",
    "C'était une partie plus complexe et très intéressante :\n",
    "\n",
    "1.  **Identification des mots distincts** : La fonction `distinct_words` a permis de lister tous les mots uniques du corpus et d'en obtenir le compte. C'est la base pour construire notre matrice.\n",
    "2.  **Calcul de la matrice de co-occurrence** : Nous avons commencé à implémenter `compute_co_occurrence_matrix`. J'ai compris le concept de fenêtre (`window_size`) pour déterminer quels mots co-occurrences. L'exemple avec les documents \"all that glitters is not gold\" et \"all is well that ends well\" a bien illustré comment construire cette matrice symétrique. *Il semble qu'il y ait un début d'implémentation pour le remplissage de la matrice, mais elle n'est pas encore complète pour calculer toutes les co-occurrences dans la fenêtre.* Une fois que cette partie sera complétée, la matrice `M` montrera combien de fois chaque mot apparaît à proximité d'un autre.\n",
    "3.  **Réduction de la dimensionnalité (SVD)** : La fonction `reduce_to_k_dim` utilise `TruncatedSVD` pour réduire les dimensions de la matrice de co-occurrence à `k` dimensions (ici, `k=2`). C'est crucial car cela permet de condenser l'information tout en conservant les relations sémantiques. L'idée est que des mots qui apparaissent souvent ensemble auront des vecteurs similaires même après réduction.\n",
    "4.  **Normalisation** : Après la réduction, la normalisation (`M_normalized`) rend les vecteurs de longueur unitaire, ce qui facilite la comparaison de similarité via le produit scalaire. *J'ai remarqué une `RuntimeWarning: invalid value encountered in divide` lors de la normalisation, ce qui suggère qu'il pourrait y avoir des lignes de zéros dans `M_reduced_co_occurrence` ou `M_lengths`, ce qui est à vérifier dans l'implémentation de `compute_co_occurrence_matrix` ou `reduce_to_k_dim`.*\n",
    "5.  **Recherche de similarité** : La fonction `most_similar` a été initiée pour trouver les mots les plus proches en utilisant le produit scalaire. *Cette fonction semble être incomplète et nécessite d'être finalisée pour comparer correctement les vecteurs.*\n",
    "\n",
    "#### Encadrage GloVe (Pretrained)\n",
    "Nous avons exploré les embeddings pré-entraînés GloVe, chargés via `gensim.downloader`. C'était très impressionnant de voir comment un modèle pré-entraîné sur de vastes corpus capture des relations sémantiques complexes :\n",
    "*   `model.most_similar(positive=[\"fox\", \"rabbit\", \"cat\"])` : Les résultats montrent des animaux similaires (\"dog\", \"mouse\", \"wolf\", \"bunny\"). Cela démontre que les embeddings GloVe capturent bien la similarité thématique.\n",
    "*   `model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])` : L'exemple d'analogie \"king - man + woman = queen\" fonctionne parfaitement, prouvant la capacité des embeddings à capturer des relations analogiques (ici, la relation de genre).\n",
    "\n",
    "Cette section a mis en évidence la puissance des embeddings pré-entraînés pour gérer les synonymes, les analogies, et de manière générale, la sémantique. Les biais mentionnés (synonymes, antonymes, polysemie, ironie, analogies) viennent probablement des données sur lesquelles les modèles sont entraînés. Si un mot est souvent utilisé dans un contexte négatif, cela peut influencer son embedding.\n",
    "\n",
    "### Partie II - Représentations de phrases\n",
    "\n",
    "#### TF-IDF\n",
    "Nous avons commencé à travailler sur la représentation de phrases avec TF-IDF. L'idée est de pondérer l'importance d'un mot dans un document par sa fréquence dans ce document (TF) et son inverse de fréquence dans l'ensemble du corpus (IDF). Cela permet de mettre en lumière les mots distinctifs d'un document.\n",
    "\n",
    "*   Nous avons converti le corpus en une liste de chaînes de caractères (`corpus_string`) pour l'utiliser avec `TfidfVectorizer` de scikit-learn.\n",
    "*   La fonction `search_corpus` utilise `TfidfVectorizer` pour transformer le corpus et une requête de recherche en vecteurs TF-IDF. Ensuite, elle calcule la similarité par produit scalaire et retourne les documents les plus pertinents.\n",
    "*   L'exécution de `search_corpus(corpus, \"europe\", topn=5)` a montré des résultats très pertinents, avec des articles mentionnant l'Europe, les importations, le pétrole, etc. Cela prouve que TF-IDF est efficace pour la recherche d'informations et l'identification de documents thématiquement proches d'une requête.\n",
    "\n",
    "#### Unsupervised Random Walk Sentence Embeddings (uSIF)\n",
    "Cette partie est une introduction à une méthode plus avancée pour les embeddings de phrases. L'idée de combiner des word embeddings existants avec SVD pour obtenir des embeddings de phrases est prometteuse, et la référence à l'implémentation et à l'article est un bon point de départ pour l'explorer davantage. L'objectif serait de comparer ses performances avec TF-IDF et l'agrégation simple de Word2Vec.\n",
    "\n",
    "En résumé, ce notebook nous a permis de voir la progression des méthodes de représentation textuelle, du simple one-hot encoding aux embeddings sophistiqués comme GloVe et TF-IDF, en passant par les matrices de co-occurrence. Chaque méthode a ses forces et ses faiblesses, et le choix dépend vraiment du cas d'utilisation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
